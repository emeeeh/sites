### Combined Content for cluster-nodes ###


================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/

Cluster nodes # Cluster nodes are node groups that work together to provide functionality in an n8n workflow. Instead of using a single node, you use a root node and one or more sub-nodes that extend the functionality of the node. Root nodes # Each cluster starts with one root node . Sub-nodes # Each root node can have one or more sub-nodes attached to it. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmollama/common-issues/

Ollama Model node common issues # Here are some common errors and issues with the Ollama Model node and steps to resolve or troubleshoot them. Processing parameters # The Ollama Model node is a sub-node . Sub-nodes behave differently than other nodes when processing multiple items using expressions. Most nodes, including root nodes , take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Can't connect to a remote Ollama instance # The Ollama Model node is only designed to connect to a locally hosted Ollama instance. It doesn't include the authentication features you'd need to connect to a remotely hosted Ollama instance. To use the Ollama Model, follow the Ollama credentials instructions to set up Ollama locally and configure the instance URL in n8n. Can't connect to a local Ollama instance when using Docker # The Ollama Model node connects to a locally hosted Ollama instance using the base URL defined by Ollama credentials . When you run either n8n or Ollama in Docker, you need to configure the network so that n8n can connect to Ollama. Ollama typically listens for connections on localhost , the local network address. In Docker, by default, each container has its own localhost which is only accessible from within the container. If either n8n or Ollama are running in containers, they won't be able to connect over localhost . The solution depends on how you're hosting the two components. If only Ollama is in Docker # If only Ollama is running in Docker, configure Ollama to listen on all interfaces by binding to 0.0.0.0 inside of the container (the official images are already configured this way). When running the container, publish the ports with the -p flag. By default, Ollama runs on port 11434, so your Docker command should look like this: 1 docker run -d -v ollama:/root/.ollama -p 11434 :11434 --name ollama ollama/ollama When configuring Ollama credentials , the localhost address should work without a problem (set the base URL to http://localhost:11434 ). If only n8n is in Docker # If only n8n is running in Docker, configure Ollama to listen on all interfaces by binding to 0.0.0.0 on the host. If you are running n8n in Docker on Linux , use the --add-host flag to map host.docker.internal to host-gateway when you start the container. For example: 1 docker run -it --rm --add-host host.docker.internal:host-gateway --name n8n -p 5678 :5678 -v n8n_data:/home/node/.n8n docker.n8n.io/n8nio/n8n If you are using Docker Desktop, this is automatically configured for you. When configuring Ollama credentials , use host.docker.internal as the host address instead of localhost . For example, to bind to the default port 11434, you could set the base URL to http://host.docker.internal:11434 . If Ollama and n8n are running in separate Docker containers # If both n8n and Ollama are running in Docker in separate containers, you can use Docker networking to connect them. Configure Ollama to listen on all interfaces by binding to 0.0.0.0 inside of the container (the official images are already configured this way). When configuring Ollama credentials , use the Ollama container's name as the host address instead of localhost . For example, if you call the Ollama container my-ollama and it listens on the default port 11434, you would set the base URL to http://my-ollama:11434 . If Ollama and n8n are running in the same Docker container # If Ollama and n8n are running in the same Docker container, the localhost address doesn't need any special configuration. You can configure Ollama to listen on localhost and configure the base URL in the Ollama credentials in n8n to use localhost: http://localhost:11434 . Error: connect ECONNREFUSED ::1:11434 # This error occurs when your computer has IPv6 enabled, but Ollama is listening to an IPv4 address. To fix this, change the base URL in your Ollama credentials to connect to 127.0.0.1 , the IPv4-specific local address, instead of the localhost alias that can resolve to either IPv4 or IPv6: http://127.0.0.1:11434 . Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/common-issues/

AI Agent node common issues # Here are some common errors and issues with the AI Agent node and steps to resolve or troubleshoot them. Internal error: 400 Invalid value for 'content' # A full error message might look like this: 1 2 3 Internal error Error: 400 Invalid value for 'content': expected a string, got null. <stack-trace> This error can occur if the Prompt input contains a null value. You might see this in one of two scenarios: When you've set the Prompt to Define below and have an expression in your Text that isn't generating a value. To resolve, make sure your expressions reference valid fields and that they resolve to valid input rather than null. When you've set the Prompt to Connected Chat Trigger Node and the incoming data has null values. To resolve, remove any null values from the chatInput field of the input node. Error in sub-node Window Buffer Memory # This error displays when n8n runs into an issue with the Window Buffer Memory sub-node. It most often occurs when your workflow or the workflow template you copied uses an older version of the Window Buffer Memory node. Try removing the Window Buffer Memory node from your workflow and re-adding it, which will guarantee you're using the latest version of the node. A Chat Model sub-node must be connected error # This error displays when n8n tries to execute the node without having a Chat Model connected. To resolve this, click the + Chat Model button at the bottom of your screen when the node is open, or click the Chat Model + connector when the node is closed. n8n will then open a selection of possible Chat Models to pick from. No prompt specified error # This error occurs when the agent expects to get the prompt from the previous node automatically. Typically, this happens when you're using the Chat Trigger Node . To resolve this issue, find the Prompt parameter of the AI Agent node and change it from Connected Chat Trigger Node to Define below . This allows you to manually build your prompt by referencing output data from other nodes or by adding static text. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.chainretrievalqa/common-issues/

Question and Answer Chain node common issues # Here are some common errors and issues with the Question and Answer Chain node and steps to resolve or troubleshoot them. No prompt specified error # This error displays when the Prompt is empty or invalid. You might see this in one of two scenarios: When you've set the Prompt to Define below and have an expression in your Text that isn't generating a value. To resolve, enter a valid prompt in the Text field. Make sure any expressions reference valid fields and that they resolve to valid input rather than null. When you've set the Prompt to Connected Chat Trigger Node and the incoming data has null values. To resolve, make sure your input contains a chatInput field. Add an Edit Fields (Set) node to edit an incoming field name to chatInput . Remove any null values from the chatInput field of the input node. A Retriever sub-node must be connected error # This error displays when n8n tries to execute the node without having a Retriever connected. To resolve this, click the + Retriever button at the bottom of your screen when the node is open, or click the Retriever + connector when the node isn't open. n8n will then open a selection of possible Retrievers to pick from. Can't produce longer responses # If you need to generate longer responses than the Question and Answer Chain node produces by default, you can try one or more of the following techniques: Connect a more verbose model : Some AI models produce more terse results than others. Swapping your model for one with a larger context window and more verbose output can increase the word length of your responses. Increase the maximum number of tokens : Many model nodes (for example the OpenAI Chat Model ) include a Maximum Number of Tokens option. You can set this to increase the maximum number of tokens the model can use to produce a response. Build larger responses in stages : For more detailed answers, you may want to construct replies in stages using a variety of AI nodes. You can use AI split up a single question into multiple prompts and create responses for each. You can then compose a final reply by combining the responses again. Though the details are different, you can find a good example of the general idea in this template for writing a WordPress post with AI . Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.outputparserstructured/common-issues/

Structured Output Parser node common issues # Here are some common errors and issues with the Structured Output Parser node and steps to resolve or troubleshoot them. Processing parameters # The Structured Output Parser node is a sub-node . Sub-nodes behave differently than other nodes when processing multiple items using expressions. Most nodes, including root nodes , take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Adding the structured output parser node to AI nodes # You can attach output parser nodes to select AI root nodes . To add the Structured Output Parser to a node, enable the Require Specific Output Format option in the AI root node you wish to format. Once the option is enabled, a new output parser attachment point is displayed. Click the output parser attachment point to add the Structured Output Parser node to the node. Using the structured output parser to format intermediary steps # The Structured Output Parser node structures the final output from AI agents. It's not intended to structure intermediary output to pass to other AI tools or stages. To request a specific format for intermediary output, include the response structure in the System Message for the AI Agent . The message can include either a schema or example response for the agent to use as a template for its results. Structuring output from agents # Structured output parsing is often not reliable when working with agents . If your workflow uses agents, n8n recommends using a separate LLM-chain to receive the data from the agent and parse it. This leads to better, more consistent results than parsing directly in the agent workflow. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatopenai/common-issues/

OpenAI Chat Model node common issues # Here are some common errors and issues with the OpenAI Chat Model node and steps to resolve or troubleshoot them. Processing parameters # The OpenAI Chat Model node is a sub-node . Sub-nodes behave differently than other nodes when processing multiple items using expressions. Most nodes, including root nodes , take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. The service is receiving too many requests from you # This error displays when you've exceeded OpenAI's rate limits . There are two ways to work around this issue: Split your data up into smaller chunks using the Loop Over Items node and add a Wait node at the end for a time amount that will help. Copy the code below and paste it into a workflow to use as a template. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 { "nodes": [ { "parameters": {}, "id": "35d05920-ad75-402a-be3c-3277bff7cc67", "name": "When clicking â€˜Test workflowâ€™", "type": "n8n-nodes-base.manualTrigger", "typeVersion": 1, "position": [ 880, 400 ] }, { "parameters": { "batchSize": 500, "options": {} }, "id": "ae9baa80-4cf9-4848-8953-22e1b7187bf6", "name": "Loop Over Items", "type": "n8n-nodes-base.splitInBatches", "typeVersion": 3, "position": [ 1120, 420 ] }, { "parameters": { "resource": "chat", "options": {}, "requestOptions": {} }, "id": "a519f271-82dc-4f60-8cfd-533dec580acc", "name": "OpenAI", "type": "n8n-nodes-base.openAi", "typeVersion": 1, "position": [ 1380, 440 ] }, { "parameters": { "unit": "minutes" }, "id": "562d9da3-2142-49bc-9b8f-71b0af42b449", "name": "Wait", "type": "n8n-nodes-base.wait", "typeVersion": 1, "position": [ 1620, 440 ], "webhookId": "714ab157-96d1-448f-b7f5-677882b92b13" } ], "connections": { "When clicking â€˜Test workflowâ€™": { "main": [ [ { "node": "Loop Over Items", "type": "main", "index": 0 } ] ] }, "Loop Over Items": { "main": [ null, [ { "node": "OpenAI", "type": "main", "index": 0 } ] ] }, "OpenAI": { "main": [ [ { "node": "Wait", "type": "main", "index": 0 } ] ] }, "Wait": { "main": [ [ { "node": "Loop Over Items", "type": "main", "index": 0 } ] ] } }, "pinData": {} } Use the HTTP Request node with the built-in batch-limit option against the OpenAI API instead of using the OpenAI node. Insufficient quota # Quota issues There are a number of OpenAI issues surrounding quotas, including failures when quotas have been recently topped up. To avoid these issues, ensure that there is credit in the account and issue a new API key from the API keys screen . This error displays when your OpenAI account doesn't have enough credits or capacity to fulfill your request. This may mean that your OpenAI trial period has ended, that your account needs more credit, or that you've gone over a usage limit. To troubleshoot this error, on your OpenAI settings page: Select the correct organization for your API key in the first selector in the upper-left corner. Select the correct project for your API key in the second selector in the upper-left corner. Check the organization-level billing overview page to ensure that the organization has enough credit. Double-check that you select the correct organization for this page. Check the organization-level usage limits page. Double-check that you select the correct organization for this page and scroll to the Usage limits section to verify that you haven't exceeded your organization's usage limits. Check your OpenAI project's usage limits. Double-check that you select the correct project in the second selector in the upper-left corner. Select Project > Limits to view or change the project limits. Check that the OpenAI API is operating as expected. Balance waiting period After topping up your balance, there may be a delay before your OpenAI account reflects the new balance. In n8n: check that the OpenAI credentials use a valid OpenAI API key for the account you've added money to ensure that you connect the OpenAI node to the correct OpenAI credentials If you find yourself frequently running out of account credits, consider turning on auto recharge in your OpenAI billing settings to automatically reload your account with credits when your balance reaches $0. Bad request - please check your parameters # This error displays when the request results in an error but n8n wasn't able to interpret the error message from OpenAI. To begin troubleshooting, try running the same operation using the HTTP Request node, which should provide a more detailed error message. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memorybufferwindow/common-issues/

Window Buffer Memory node common issues # Here are some common errors and issues with the Window Buffer Memory node and steps to resolve or troubleshoot them. Single memory instance # If you add more than one Window Buffer Memory node to your workflow, all nodes access the same memory instance by default. Be careful when doing destructive actions that override existing memory contents, such as the override all messages operation in the Chat Memory Manager node. If you want more than one memory instance in your workflow, set different session IDs in different memory nodes. Managing the Session ID # In most cases, the sessionId is automatically retrieved from the On Chat Message trigger. But you may run into an error with the phrase No sessionId . If you have this error, first check the output of your Chat trigger to ensure it includes a sessionId . If you're not using the On Chat Message trigger, you'll need to manage sessions manually. For testing purposes, you can use a static key like my_test_session . If you use this approach, be sure to set up proper session management before activating the workflow to avoid potential issues in a live environment. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatollama/common-issues/

Ollama Chat Model node common issues # Here are some common errors and issues with the Ollama Chat Model node and steps to resolve or troubleshoot them. Processing parameters # The Ollama Chat Model node is a sub-node . Sub-nodes behave differently than other nodes when processing multiple items using expressions. Most nodes, including root nodes , take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Can't connect to a remote Ollama instance # The Ollama Chat Model node is only designed to connect to a locally hosted Ollama instance. It doesn't include the authentication features you'd need to connect to a remotely hosted Ollama instance. To use the Ollama Chat Model, follow the Ollama credentials instructions to set up Ollama locally and configure the instance URL in n8n. Can't connect to a local Ollama instance when using Docker # The Ollama Chat Model node connects to a locally hosted Ollama instance using the base URL defined by Ollama credentials . When you run either n8n or Ollama in Docker, you need to configure the network so that n8n can connect to Ollama. Ollama typically listens for connections on localhost , the local network address. In Docker, by default, each container has its own localhost which is only accessible from within the container. If either n8n or Ollama are running in containers, they won't be able to connect over localhost . The solution depends on how you're hosting the two components. If only Ollama is in Docker # If only Ollama is running in Docker, configure Ollama to listen on all interfaces by binding to 0.0.0.0 inside of the container (the official images are already configured this way). When running the container, publish the ports with the -p flag. By default, Ollama runs on port 11434, so your Docker command should look like this: 1 docker run -d -v ollama:/root/.ollama -p 11434 :11434 --name ollama ollama/ollama When configuring Ollama credentials , the localhost address should work without a problem (set the base URL to http://localhost:11434 ). If only n8n is in Docker # If only n8n is running in Docker, configure Ollama to listen on all interfaces by binding to 0.0.0.0 on the host. If you are running n8n in Docker on Linux , use the --add-host flag to map host.docker.internal to host-gateway when you start the container. For example: 1 docker run -it --rm --add-host host.docker.internal:host-gateway --name n8n -p 5678 :5678 -v n8n_data:/home/node/.n8n docker.n8n.io/n8nio/n8n If you are using Docker Desktop, this is automatically configured for you. When configuring Ollama credentials , use host.docker.internal as the host address instead of localhost . For example, to bind to the default port 11434, you could set the base URL to http://host.docker.internal:11434 . If Ollama and n8n are running in separate Docker containers # If both n8n and Ollama are running in Docker in separate containers, you can use Docker networking to connect them. Configure Ollama to listen on all interfaces by binding to 0.0.0.0 inside of the container (the official images are already configured this way). When configuring Ollama credentials , use the Ollama container's name as the host address instead of localhost . For example, if you call the Ollama container my-ollama and it listens on the default port 11434, you would set the base URL to http://my-ollama:11434 . If Ollama and n8n are running in the same Docker container # If Ollama and n8n are running in the same Docker container, the localhost address doesn't need any special configuration. You can configure Ollama to listen on localhost and configure the base URL in the Ollama credentials in n8n to use localhost: http://localhost:11434 . Error: connect ECONNREFUSED ::1:11434 # This error occurs when your computer has IPv6 enabled, but Ollama is listening to an IPv4 address. To fix this, change the base URL in your Ollama credentials to connect to 127.0.0.1 , the IPv4-specific local address, instead of the localhost alias that can resolve to either IPv4 or IPv6: http://127.0.0.1:11434 . Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/conversational-agent/

Conversational AI Agent node # The Conversational Agent has human-like conversations. It can maintain context, understand user intent, and provide relevant answers. This agent is typically used for building chatbots, virtual assistants, and customer support systems. The Conversational Agent describes tools in the system prompt and parses JSON responses for tool calls. If your preferred AI model doesn't support tool calling or you're handling simpler interactions, this agent is a good general option. It's more flexible but may be less accurate than the Tools Agent . Refer to AI Agent for more information on the AI Agent node itself. You can use this agent with the Chat Trigger node. Attach a memory sub-node so that users can have an ongoing conversation with multiple queries. Memory doesn't persist between sessions. Node parameters # Configure the Conversational Agent using the following parameters. Prompt # Select how you want the node to construct the prompt (also known as the user's query or input from the chat). Choose from: Take from previous node automatically : If you select this option, the node expects an input from a previous node called chatInput . Define below : If you select this option, provide either static text or an expression for dynamic content to serve as the prompt in the Prompt (User Message) field. Require Specific Output Format # This parameter controls whether you want the node to require a specific output format. When turned on, n8n prompts you to connect one of these output parsers to the node: Auto-fixing Output Parser Item List Output Parser Structured Output Parser Node options # Refine the Conversational Agent node's behavior using these options: Human Message # Tell the agent about the tools it can use and add context to the user's input. You must include these expressions and variable: {tools} : A LangChain expression that provides a string of the tools you've connected to the Agent. Provide some context or explanation about who should use the tools and how they should use them. {format_instructions} : A LangChain expression that provides the schema or format from the output parser node you've connected. Since the instructions themselves are context, you don't need to provide context for this expression. {{input}} : A LangChain variable containing the user's prompt. This variable populates with the value of the Prompt parameter. Provide some context that this is the user's input. Here's an example of how you might use these strings: Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 TOOLS ------ Assistant can ask the user to use tools to look up information that may be helpful in answering the user's original question. The tools the human can use are: {tools} {format_instructions} USER'S INPUT -------------------- Here is the user's input (remember to respond with a markdown code snippet of a JSON blob with a single action, and NOTHING else): {{input}} System Message # If you'd like to send a message to the agent before the conversation starts, enter the message you'd like to send. Use this option to guide the agent's decision-making. Max Iterations # Enter the number of times the model should run to try and generate a good answer from the user's prompt. Defaults to 10 . Return Intermediate Steps # Select whether to include intermediate steps the agent took in the final output (turned on) or not (turned off). This could be useful for further refining the agent's behavior based on the steps it took. Templates and examples # Refer to the main AI Agent node's Templates and examples section. Common issues # For common questions or issues and suggested solutions, refer to Common issues . AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/

AI Agent node # An AI agent is an autonomous system that receives data, makes rational decisions, and acts within its environment to achieve specific goals. The AI agent's environment is everything the agent can access that isn't the agent itself, which might include customer queries, tools, or other context. The AI Agent node gives you six LangChain agent options. On this page, you'll find a summary of the available agents with links to more detailed pages on each, templates and examples, and links to more resources. Connect a tool You must connect at least one tool sub-node to an AI Agent node. Available agents # n8n provides these agents: Tools Agent (default): This agent uses external tools and APIs to perform actions and retrieve information. It can understand the capabilities of different tools and determine which tool to use depending on the task. This agent helps integrate LLMs with various external services and databases. Begin here n8n recommends using this agent for most use cases. It will handle most scenarios and provides the best experience when working with tools . Conversational Agent : This agent has human-like conversations. It can maintain context, understand user intent, and provide relevant answers. This agent is typically used for building chatbots, virtual assistants and customer support systems. If your preferred AI model doesn't support tool calling or you're handling simpler interactions, this agent is a good option. OpenAI Functions Agent : Use this agent with an OpenAI functions model . You must use the OpenAI Chat Model with this agent. Plan and Execute Agent : This agent creates a high-level plan to solve the given task and then executes the plan step by step. It's most useful for tasks that require a structured approach and careful planning. ReAct Agent : This agent reasons about a given task, determines the necessary actions, and then executes them. It follows the cycle of reasoning and acting until it completes the task. The ReAct agent can break down complex tasks into smaller sub-tasks, prioritise them, and execute them one after the other. No memory The ReAct agent doesn't support memory sub-nodes. This means it can't recall previous prompts or simulate an ongoing conversation. SQL Agent : This agent uses a SQL database as a data source. It can understand natural language questions, convert them into SQL queries, execute the queries, and present the results in a user-friendly format. This agent is valuable for building natural language interfaces to databases. Templates and examples # AI agent chat by n8n Team View template details AI agent that can scrape webpages by Eduard View template details Building Your First WhatsApp Chatbot by Jimleuk View template details Browse AI Agent integration templates , or search all templates Related resources # Refer to LangChain's documentation on agents for more information about the service. New to AI Agents? Read the n8n blog introduction to AI agents . View n8n's Advanced AI documentation. Common issues # For common errors or issues and suggested resolution steps, refer to Common Issues . AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.chainllm/

Basic LLM Chain node # Use the Basic LLM Chain node to set the prompt that the model will use along with setting an optional parser for the response. On this page, you'll find the node parameters for the Basic LLM Chain node and links to more resources. Examples and templates For usage examples and templates to help you get started, refer to n8n's Basic LLM Chain integrations page. Node parameters # Prompt # Select how you want the node to construct the prompt (also known as the user's query or input from the chat). Choose from: Take from previous node automatically : If you select this option, the node expects an input from a previous node called chatInput . Define below : If you select this option, provide either static text or an expression for dynamic content to serve as the prompt in the Prompt (User Message) field. Require Specific Output Format # This parameter controls whether you want the node to require a specific output format. When turned on, n8n prompts you to connect one of these output parsers to the node: Auto-fixing Output Parser Item List Output Parser Structured Output Parser Chat Messages # Use Chat Messages when you're using a chat model to set a message. n8n ignores these options if you don't connect a chat model. Select the Type Name or ID you want the node to use: AI # Enter a sample expected response in the Message field. The model will try to respond in the same way in its messages. System # Enter a system Message to include with the user input to help guide the model in what it should do. Use this option for things like defining tone, for example: Always respond talking like a pirate . User # Enter a sample user input. Using this with the AI option can help improve the output of the agent. Using both together provides a sample of an input and expected response (the AI Message ) for the model to follow. Select one of these input types: Text : Enter a sample user input as a text Message . Image (Binary) : Select a binary input from a previous node. Enter the Image Data Field Name to identify which binary field from the previous node contains the image data. Image (URL) : Use this option to feed an image in from a URL. Enter the Image URL . For both the Image types, select the Image Details to control how the model processes the image and generates its textual understanding. Choose from: Auto : The model uses the auto setting, which looks at the image input size and decide if it should use the Low or High setting. Low : The model receives a low-resolution 512px x 512px version of the image and represents the image with a budget of 65 tokens. This allows the API to return faster responses and consume fewer input tokens. Use this option for use cases that don't require high detail. High : The model can access the low-resolution image and then creates detailed crops of input images as 512px squares based on the input image size. Each of the detailed crops uses twice the token budget (65 tokens) for a total of 129 tokens. Use this option for use cases that require high detail. Templates and examples # Chat with PDF docs using AI (quoting sources) by David Roberts View template details Chat with local LLMs using n8n and Ollama by Mihai Farcas View template details Suggest meeting slots using AI by n8n Team View template details Browse Basic LLM Chain integration templates , or search all templates Related resources # Refer to LangChain's documentation on Basic LLM Chains for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Common issues # Here are some common errors and issues with the Basic LLM Chain node and steps to resolve or troubleshoot them. No prompt specified error # This error displays when the Prompt is empty or invalid. You might see this error in one of two scenarios: When you've set the Prompt to Define below and haven't entered anything in the Text field. To resolve, enter a valid prompt in the Text field. When you've set the Prompt to Connected Chat Trigger Node and the incoming data has no field called chatInput . The node expects the chatInput field. If your previous node doesn't have this field, add an Edit Fields (Set) node to edit an incoming field name to chatInput . Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.chainretrievalqa/

Question and Answer Chain node # Use the Question and Answer Chain node to use a vector store as a retriever. On this page, you'll find the node parameters for the Question and Answer Chain node, and links to more resources. Node parameters # Query # The question you want to ask. Templates and examples # Ask questions about a PDF using AI by David Roberts View template details AI Crew to Automate Fundamental Stock Analysis - Q&A Workflow by Derek Cheung View template details Advanced AI Demo (Presented at AI Developers #14 meetup) by Max Tkacz View template details Browse Question and Answer Chain integration templates , or search all templates Related resources # Refer to LangChain's documentation on retrieval chains for examples of how LangChain can use a vector store as a retriever. View n8n's Advanced AI documentation. Common issues # For common errors or issues and suggested resolution steps, refer to Common Issues . AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.chainsummarization/

Summarization Chain node # Use the Summarization Chain node to summarize multiple documents. On this page, you'll find the node parameters for the Summarization Chain node, and links to more resources. Node parameters # Choose the type of data you need to summarize in Data to Summarize . The data type you choose determines the other node parameters. Use Node Input (JSON) and Use Node Input (Binary) : summarize the data coming into the node from the workflow. You can configure the Chunking Strategy : choose what strategy to use to define the data chunk sizes. If you choose Simple (Define Below) you can then set Characters Per Chunk and Chunk Overlap (Characters) . Choose Advanced if you want to connect a splitter sub-node that provides more configuration options. Use Document Loader : summarize data provided by a document loader sub-node. Node Options # You can configure the summarization method and prompts. Select Add Option > Summarization Method and Prompts . Options in Summarization Method : Map Reduce : this is the recommended option. Learn more about Map Reduce in the LangChain documentation. Refine : learn more about Refine in the LangChain documentation. Stuff : learn more about Stuff in the LangChain documentation. You can customize the Individual Summary Prompts and the Final Prompt to Combine . There are examples in the node. You must include the "{text}" placeholder. Templates and examples # Scrape and summarize webpages with AI by n8n Team View template details âš¡AI-Powered YouTube Video Summarization & Analysis by Joseph LePage View template details AI Automated HR Workflow for CV Analysis and Candidate Evaluation by n3w Italia View template details Browse Summarization Chain integration templates , or search all templates Related resources # Refer to LangChain's documentation on summarization for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.code/

LangChain Code node # Use the LangChain Code node to import LangChain. This means if there is functionality you need that n8n hasn't created a node for, you can still use it. By configuring the LangChain Code node connectors you can use it as a normal node, root node or sub-node. On this page, you'll find the node parameters, guidance on configuring the node, and links to more resources. Not available on Cloud This node is only available on self-hosted n8n. Node parameters # Add Code # Add your custom code. Choose either Execute or Supply Data mode. You can only use one mode. Unlike the Code node , the LangChain Code node doesn't support Python. Execute : use the LangChain Code node like n8n's own Code node. This takes input data from the workflow, processes it, and returns it as the node output. This mode requires a main input and output. You must create these connections in Inputs and Outputs . Supply Data : use the LangChain Code node as a sub-node, sending data to a root node. This uses an output other than main. By default, you can't load built-in or external modules in this node. Self-hosted users can enable built-in and external modules . Inputs # Choose the input types. The main input is the normal connector found in all n8n workflows. If you have a main input and output set in the node, Execute code is required. Outputs # Choose the output types. The main output is the normal connector found in all n8n workflows. If you have a main input and output set in the node, Execute code is required. Node inputs and outputs configuration # By configuring the LangChain Code node connectors (inputs and outputs) you can use it as an app node, root node or sub-node. Node type Inputs Outputs Code mode App node. Similar to the Code node . Main Main Execute Root node Main; at least one other type Main Execute Sub-node - A type other than main. Must match the input type you want to connect to. Supply Data Sub-node with sub-nodes A type other than main A type other than main. Must match the input type you want to connect to. Supply Data Built-in methods # n8n provides these methods to make it easier to perform common tasks in the LangChain Code node. Method Description this.addInputData(inputName, data) Populate the data of a specified non-main input. Useful for mocking data. inputName is the input connection type, and must be one of: ai_agent , ai_chain , ai_document , ai_embedding , ai_languageModel , ai_memory , ai_outputParser , ai_retriever , ai_textSplitter , ai_tool , ai_vectorRetriever , ai_vectorStore data contains the data you want to add. Refer to Data structure for information on the data structure expected by n8n. this.addOutputData(outputName, data) Populate the data of a specified non-main output. Useful for mocking data. outputName is the input connection type, and must be one of: ai_agent , ai_chain , ai_document , ai_embedding , ai_languageModel , ai_memory , ai_outputParser , ai_retriever , ai_textSplitter , ai_tool , ai_vectorRetriever , ai_vectorStore data contains the data you want to add. Refer to Data structure for information on the data structure expected by n8n. this.getInputConnectionData(inputName, itemIndex, inputIndex?) Get data from a specified non-main input. inputName is the input connection type, and must be one of: ai_agent , ai_chain , ai_document , ai_embedding , ai_languageModel , ai_memory , ai_outputParser , ai_retriever , ai_textSplitter , ai_tool , ai_vectorRetriever , ai_vectorStore itemIndex should always be 0 (this parameter will be used in upcoming functionality) Use inputIndex if there is more than one node connected to the specified input. this.getInputData(inputIndex?, inputName?) Get data from the main input. this.getNode() Get the current node. this.getNodeOutputs() Get the outputs of the current node. this.getExecutionCancelSignal() Use this to stop the execution of a function when the workflow stops. In most cases n8n handles this, but you may need to use it if building your own chains or agents. It replaces the Cancelling a running LLMChain code that you'd use if building a LangChain application normally. Templates and examples # Custom LangChain agent written in JavaScript by n8n Team View template details Use any LangChain module in n8n (with the LangChain code node) by David Roberts View template details ðŸ¤– AI Powered RAG Chatbot for Your Docs + Google Drive + Gemini + Qdrant by Joseph LePage View template details Browse LangChain Code integration templates , or search all templates Related resources # View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.documentdefaultdataloader/

Default Data Loader node # Use the Default Data Loader node to load binary data files or JSON data for vector stores or summarization. On this page, you'll find a list of parameters the Default Data Loader node supports, and links to more resources. Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Type of Data : Select Binary or JSON . Data Format : Displays when you set Type of Data to Binary . Select the file MIME type for your binary data. Set to Automatically Detect by MIME Type if you want n8n to set the data format for you. If you set a specific data format and the incoming file MIME type doesn't match it, the node errors. If you use Automatically Detect by MIME Type , the node falls back to text format if it can't match the file MIME type to a supported data format. Mode : Displays when you set Type of Data to JSON . Choose from: Load All Input Data : Use all the node's input data. Load Specific Data : Use expressions to define the data you want to load. You can add text as well as expressions. This means you can create a custom document from a mix of text and expressions. Node options # Metadata : Set the metadata that should accompany the document in the vector store. This is what you match to using the Metadata Filter option when retrieving data using the vector store nodes. Templates and examples # Scrape and summarize webpages with AI by n8n Team View template details Building Your First WhatsApp Chatbot by Jimleuk View template details Chat with PDF docs using AI (quoting sources) by David Roberts View template details Browse Default Data Loader integration templates , or search all templates Related resources # Refer to LangChain's documentation on document loaders for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.documentgithubloader/

GitHub Document Loader node # Use the GitHub Document Loader node to load data from a GitHub repository for vector stores or summarization. On this page, you'll find the node parameters for the GitHub Document Loader node, and links to more resources. Credentials You can find authentication information for this node here . This node doesn't support OAuth for authentication. Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Repository Link : Enter the URL of your GitHub repository. Branch : Enter the branch name to use. Node options # Recursive : Select whether to include sub-folders and files (turned on) or not (turned off). Ignore Paths : Enter directories to ignore. Templates and examples # Back Up Your n8n Workflows To Github by Jonathan View template details Building RAG Chatbot for Movie Recommendations with Qdrant and Open AI by Jenny View template details Chat with GitHub API Documentation: RAG-Powered Chatbot with Pinecone & OpenAI by Mihai Farcas View template details Browse GitHub Document Loader integration templates , or search all templates Related resources # Refer to LangChain's documentation on document loaders for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.embeddingsawsbedrock/

Embeddings AWS Bedrock node # Use the Embeddings AWS Bedrock node to generate embeddings for a given text. On this page, you'll find the node parameters for the Embeddings AWS Bedrock node, and links to more resources. Credentials You can find authentication information for this node here . Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Model : Select the model to use to generate the embedding. Learn more about available models in the Amazon Bedrock documentation . Templates and examples # Building Your First WhatsApp Chatbot by Jimleuk View template details Ask questions about a PDF using AI by David Roberts View template details Chat with PDF docs using AI (quoting sources) by David Roberts View template details Browse Embeddings AWS Bedrock integration templates , or search all templates Related resources # Refer to LangChains's AWS Bedrock embeddings documentation and the AWS Bedrock documentation for more information about AWS Bedrock. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.embeddingsazureopenai/

Embeddings Azure OpenAI node # Use the Embeddings Azure OpenAI node to generate embeddings for a given text. On this page, you'll find the node parameters for the Embeddings Azure OpenAI node, and links to more resources. Credentials You can find authentication information for this node here . Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node options # Model (Deployment) Name : Select the model (deployment) to use for generating embeddings. Batch Size : Enter the maximum number of documents to send in each request. Strip New Lines : Select whether to remove new line characters from input text (turned on) or not (turned off). n8n enables this by default. Timeout : Enter the maximum amount of time a request can take in seconds. Set to -1 for no timeout. Templates and examples # Building Your First WhatsApp Chatbot by Jimleuk View template details Ask questions about a PDF using AI by David Roberts View template details Chat with PDF docs using AI (quoting sources) by David Roberts View template details Browse Embeddings Azure OpenAI integration templates , or search all templates Related resources # Refer to LangChains's OpenAI embeddings documentation for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.embeddingscohere/

Embeddings Cohere node # Use the Embeddings Cohere node to generate embeddings for a given text. On this page, you'll find the node parameters for the Embeddings Cohere node, and links to more resources. Credentials You can find authentication information for this node here . Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Model : Select the model to use to generate the embedding. Choose from: Embed-English-v2.0(4096 Dimensions) Embed-English-Light-v2.0(1024 Dimensions) Embed-Multilingual-v2.0(768 Dimensions) Learn more about available models in Cohere's models documentation . Templates and examples # Building Your First WhatsApp Chatbot by Jimleuk View template details Ask questions about a PDF using AI by David Roberts View template details Chat with PDF docs using AI (quoting sources) by David Roberts View template details Browse Embeddings Cohere integration templates , or search all templates Related resources # Refer to Langchain's Cohere embeddings documentation for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.embeddingsgooglegemini/

Embeddings Google Gemini node # Use the Embeddings Google Gemini node to generate embeddings for a given text. On this page, you'll find the node parameters for the Embeddings Google Gemini node, and links to more resources. Credentials You can find authentication information for this node here . Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Model : Select the model to use to generate the embedding. Learn more about available models in Google Gemini's models documentation . Templates and examples # RAG Chatbot for Company Documents using Google Drive and Gemini by Mihai Farcas View template details API Schema Extractor by Polina Medvedieva View template details ðŸ¤– AI Powered RAG Chatbot for Your Docs + Google Drive + Gemini + Qdrant by Joseph LePage View template details Browse Embeddings Google Gemini integration templates , or search all templates Related resources # Refer to Langchain's Google Generative AI embeddings documentation for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.embeddingsgooglepalm/

Embeddings Google PaLM node # Use the Embeddings Google PaLM node to generate embeddings for a given text. On this page, you'll find the node parameters for the Embeddings Google PaLM node, and links to more resources. Credentials You can find authentication information for this node here . Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Model : Select the model to use to generate the embedding. n8n dynamically loads models from the Google PaLM API and you'll only see the models available to your account. Templates and examples # Ask questions about a PDF using AI by David Roberts View template details Chat with PDF docs using AI (quoting sources) by David Roberts View template details RAG Chatbot for Company Documents using Google Drive and Gemini by Mihai Farcas View template details Browse Embeddings Google PaLM integration templates , or search all templates Related resources # Refer to Langchain's Google PaLM embeddings documentation for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.embeddingshuggingfaceinference/

Embeddings HuggingFace Inference node # Use the Embeddings HuggingFace Inference node to generate embeddings for a given text. On this page, you'll find the node parameters for the Embeddings HuggingFace Inference, and links to more resources. Credentials You can find authentication information for this node here . Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Model : Select the model to use to generate the embedding. Refer to the Hugging Face models documentation for available models. Node options # Custom Inference Endpoint : Enter the URL of your deployed model, hosted by HuggingFace. If you set this, n8n ignores the Model Name . Refer to HuggingFace's guide to inference for more information. Templates and examples # Building Your First WhatsApp Chatbot by Jimleuk View template details Ask questions about a PDF using AI by David Roberts View template details Chat with PDF docs using AI (quoting sources) by David Roberts View template details Browse Embeddings HuggingFace Inference integration templates , or search all templates Related resources # Refer to Langchain's HuggingFace Inference embeddings documentation for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.embeddingsmistralcloud/

Embeddings Mistral Cloud node # Use the Embeddings Mistral Cloud node to generate embeddings for a given text. On this page, you'll find the node parameters for the Embeddings Mistral Cloud node, and links to more resources. Credentials You can find authentication information for this node here . Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Model : Select the model to use to generate the embedding. Learn more about available models in Mistral's models documentation . Node options # Batch Size : Enter the maximum number of documents to send in each request. Strip New Lines : Select whether to remove new line characters from input text (turned on) or not (turned off). n8n enables this by default. Templates and examples # Breakdown Documents into Study Notes using Templating MistralAI and Qdrant by Jimleuk View template details Build a Financial Documents Assistant using Qdrant and Mistral.ai by Jimleuk View template details Build a Tax Code Assistant with Qdrant, Mistral.ai and OpenAI by Jimleuk View template details Browse Embeddings Mistral Cloud integration templates , or search all templates Related resources # Refer to Langchain's Mistral embeddings documentation for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.embeddingsollama/

Embeddings Ollama node # Use the Embeddings Ollama node to generate embeddings for a given text. On this page, you'll find the node parameters for the Embeddings Ollama node, and links to more resources. Credentials You can find authentication information for this node here . Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Model : Select the model to use to generate the embedding. Choose from: all-minilm (384 Dimensions) nomic-embed-text (768 Dimensions) Learn more about available models in Ollama's models documentation . Templates and examples # Building Your First WhatsApp Chatbot by Jimleuk View template details Ask questions about a PDF using AI by David Roberts View template details Chat with PDF docs using AI (quoting sources) by David Roberts View template details Browse Embeddings Ollama integration templates , or search all templates Related resources # Refer to Langchain's Ollama embeddings documentation for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.embeddingsopenai/

Embeddings OpenAI node # Use the Embeddings OpenAI node to generate embeddings for a given text. On this page, you'll find the node parameters for the Embeddings OpenAI node, and links to more resources. Credentials You can find authentication information for this node here . Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node options # Model : Select the model to use for generating embeddings. Base URL : Enter the URL to send the request to. Use this if you are using a self-hosted OpenAI-like model. Batch Size : Enter the maximum number of documents to send in each request. Strip New Lines : Select whether to remove new line characters from input text (turned on) or not (turned off). n8n enables this by default. Timeout : Enter the maximum amount of time a request can take in seconds. Set to -1 for no timeout. Templates and examples # Building Your First WhatsApp Chatbot by Jimleuk View template details Ask questions about a PDF using AI by David Roberts View template details Chat with PDF docs using AI (quoting sources) by David Roberts View template details Browse Embeddings OpenAI integration templates , or search all templates Related resources # Refer to LangChains's OpenAI embeddings documentation for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.information-extractor/

Information Extractor node # Use the Information Extractor node to extract structured information from incoming data. On this page, you'll find the node parameters for the Information Extractor node, and links to more resources. Node parameters # Text defines the input text to extract information from. This is usually an expression that references a field from the input items. For example, this could be {{ $json.chatInput }} if the input is a chat trigger, or {{ $json.text }} if a previous node is Extract from PDF. Use Schema Type to choose how you want to describe the desired output data format. You can choose between: From Attribute Description : This option allows you to define the schema by specifying the list of attributes and their descriptions. Generate From JSON Example : Input an example JSON object to automatically generate the schema. The node uses the object property types and names. It ignores the actual values. Define Below : Manually input the JSON schema. Read the JSON Schema guides and examples for help creating a valid JSON schema. Node options # System Prompt Template : Use this option to change the system prompt that's used for the information extraction. n8n automatically appends format specification instructions to the prompt. Related resources # View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatanthropic/

Anthropic Chat Model node # Use the Anthropic Chat Model node to use Anthropic's Claude family of chat models with conversational agents . On this page, you'll find the node parameters for the Anthropic Chat Model node, and links to more resources. Credentials You can find authentication information for this node here . Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Model : Select the model that generates the completion. Choose from: Claude Claude Instant Learn more in the Anthropic model documentation . Node options # Maximum Number of Tokens : Enter the maximum number of tokens used, which sets the completion length. Sampling Temperature : Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations. Top K : Enter the number of token choices the model uses to generate the next token. Top P : Use this option to set the probability the completion should use. Use a lower value to ignore less probable options. Templates and examples # Notion AI Assistant Generator by Max Tkacz View template details Advanced AI Demo (Presented at AI Developers #14 meetup) by Max Tkacz View template details Generate SEO Seed Keywords Using AI by simonscrapes View template details Browse Anthropic Chat Model integration templates , or search all templates Related resources # Refer to LangChains's Anthropic documentation for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatawsbedrock/

AWS Bedrock Chat Model node # The AWS Bedrock Chat Model node allows you use LLM models utilising AWS Bedrock platform. On this page, you'll find the node parameters for the AWS Bedrock Chat Model node, and links to more resources. Credentials You can find authentication information for this node here . Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Model : Select the model that generates the completion. Learn more about available models in the Amazon Bedrock model documentation . Node options # Maximum Number of Tokens : Enter the maximum number of tokens used, which sets the completion length. Sampling Temperature : Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations. Templates and examples # Transcribe audio files from Cloud Storage by Lorena View template details Extract and store text from chat images using AWS S3 by Lorena View template details Sync data between Google Drive and AWS S3 by Lorena View template details Browse AWS Bedrock Chat Model integration templates , or search all templates Related resources # Refer to LangChains's AWS Bedrock Chat Model documentation for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatazureopenai/

Azure OpenAI Chat Model node # Use the Azure OpenAI Chat Model node to use OpenAI's chat models with conversational agents . On this page, you'll find the node parameters for the Azure OpenAI Chat Model node, and links to more resources. Credentials You can find authentication information for this node here . Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Model : Select the model to use to generate the completion. Node options # Frequency Penalty : Use this option to control the chances of the model repeating itself. Higher values reduce the chance of the model repeating itself. Maximum Number of Tokens : Enter the maximum number of tokens used, which sets the completion length. Response Format : Choose Text or JSON . JSON ensures the model returns valid JSON. Presence Penalty : Use this option to control the chances of the model talking about new topics. Higher values increase the chance of the model talking about new topics. Sampling Temperature : Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations. Timeout : Enter the maximum request time in milliseconds. Max Retries : Enter the maximum number of times to retry a request. Top P : Use this option to set the probability the completion should use. Use a lower value to ignore less probable options. Templates and examples # Get Daily Exercise Plan with Flex Message via LINE by lin@davoy.tech View template details Build Your Own Counseling Chatbot on LINE to Support Mental Health Conversations by lin@davoy.tech View template details AI-Powered Gratitude Reminder Workflow for LINE by lin@davoy.tech View template details Browse Azure OpenAI Chat Model integration templates , or search all templates Related resources # Refer to LangChains's Azure OpenAI documentation for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatgooglegemini/

Google Gemini Chat Model node # Use the Google Gemini Chat Model node to use Google's Gemini chat models with conversational agents. On this page, you'll find the node parameters for the Google Gemini Chat Model node, and links to more resources. Credentials You can find authentication information for this node here . Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Model : Select the model to use to generate the completion. n8n dynamically loads models from the Google Gemini API and you'll only see the models available to your account. Node options # Maximum Number of Tokens : Enter the maximum number of tokens used, which sets the completion length. Sampling Temperature : Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations. Top K : Enter the number of token choices the model uses to generate the next token. Top P : Use this option to set the probability the completion should use. Use a lower value to ignore less probable options. Safety Settings : Gemini supports adjustable safety settings. Refer to Google's Gemini API safety settings for information on the available filters and levels. Templates and examples # Respond to WhatsApp Messages with AI Like a Pro! by Jimleuk View template details RAG Chatbot for Company Documents using Google Drive and Gemini by Mihai Farcas View template details AI-Powered Social Media Content Generator & Publisher by Amjid Ali View template details Browse Google Gemini Chat Model integration templates , or search all templates Related resources # Refer to LangChain's Google Gemini documentation for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatgooglevertex/

Google Vertex Chat Model node # Use the Google Vertex AI Chat Model node to use Google's Vertex AI chat models with conversational agents . On this page, you'll find the node parameters for the Google Vertex AI Chat Model node, and links to more resources. Credentials You can find authentication information for this node here . Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Project ID : Select the project ID from your Google Cloud account to use. n8n dynamically loads projects from the Google Cloud account, but you can also enter it manually. Model Name : Select the name of the model to use to generate the completion, for example gemini-1.5-flash-001 , gemini-1.5-pro-001 , etc. Refer to Google models for a list of available models. Node options # Maximum Number of Tokens : Enter the maximum number of tokens used, which sets the completion length. Sampling Temperature : Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations. Top K : Enter the number of token choices the model uses to generate the next token. Top P : Use this option to set the probability the completion should use. Use a lower value to ignore less probable options. Safety Settings : Gemini supports adjustable safety settings. Refer to Google's Gemini API safety settings for information on the available filters and levels. Templates and examples # Browse Google Vertex Chat Model integration templates , or search all templates Related resources # Refer to LangChain's Google Vertex AI documentation for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatgroq/

Groq Chat Model node # Use the Groq Chat Model node to access Groq's large language models for conversational AI and text generation tasks. On this page, you'll find the node parameters for the Groq Chat Model node, and links to more resources. Credentials You can find authentication information for this node here . Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Model : Select the model which will generate the completion. n8n dynamically loads available models from the Groq API. Learn more in the Groq model documentation . Node options # Maximum Number of Tokens : Enter the maximum number of tokens used, which sets the completion length. Sampling Temperature : Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations. Templates and examples # Conversational Interviews with AI Agents and n8n Forms by Jimleuk View template details Telegram chat with PDF by felipe biava cataneo View template details Extract spending history from gmail to google sheet by hana View template details Browse Groq Chat Model integration templates , or search all templates Related resources # Refer to Groq's API documentation for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatmistralcloud/

Mistral Cloud Chat Model node # Use the Mistral Cloud Chat Model node to combine Mistral Cloud's chat models with conversational agents . On this page, you'll find the node parameters for the Mistral Cloud Chat Model node, and links to more resources. Credentials You can find authentication information for this node here . Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Model : Select the model to use to generate the completion. n8n dynamically loads models from Mistral Cloud and you'll only see the models available to your account. Node options # Maximum Number of Tokens : Enter the maximum number of tokens used, which sets the completion length. Sampling Temperature : Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations. Timeout : Enter the maximum request time in milliseconds. Max Retries : Enter the maximum number of times to retry a request. Top P : Use this option to set the probability the completion should use. Use a lower value to ignore less probable options. Enable Safe Mode : Enable safe mode by injecting a safety prompt at the beginning of the completion. This helps prevent the model from generating offensive content. Random Seed : Enter a seed to use for random sampling. If set, different calls will generate deterministic results. Templates and examples # Breakdown Documents into Study Notes using Templating MistralAI and Qdrant by Jimleuk View template details Build a Financial Documents Assistant using Qdrant and Mistral.ai by Jimleuk View template details Organise Your Local File Directories With AI by Jimleuk View template details Browse Mistral Cloud Chat Model integration templates , or search all templates Related resources # Refer to LangChains's Mistral documentation for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatollama/

Ollama Chat Model node # The Ollama Chat Model node allows you use local Llama 2 models with conversational agents . On this page, you'll find the node parameters for the Ollama Chat Model node, and links to more resources. Credentials You can find authentication information for this node here . Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Model : Select the model that generates the completion. Choose from: Llama2 Llama2 13B Llama2 70B Llama2 Uncensored Refer to the Ollama Models Library documentation for more information about available models. Node options # Sampling Temperature : Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations. Top K : Enter the number of token choices the model uses to generate the next token. Top P : Use this option to set the probability the completion should use. Use a lower value to ignore less probable options. Templates and examples # Chat with local LLMs using n8n and Ollama by Mihai Farcas View template details ðŸ”ðŸ¦™ðŸ¤– Private & Local Ollama Self-Hosted AI Assistant by Joseph LePage View template details ðŸ‹DeepSeek V3 Chat & R1 Reasoning Quick Start by Joseph LePage View template details Browse Ollama Chat Model integration templates , or search all templates Related resources # Refer to LangChains's Ollama Chat Model documentation for more information about the service. View n8n's Advanced AI documentation. Common issues # For common questions or issues and suggested solutions, refer to Common issues . AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Self-hosted AI Starter Kit # New to working with AI and using self-hosted n8n? Try n8n's self-hosted AI Starter Kit to get started with a proof-of-concept or demo playground using Ollama, Qdrant, and PostgreSQL. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatopenai/

OpenAI Chat Model node # Use the OpenAI Chat Model node to use OpenAI's chat models with conversational agents . On this page, you'll find the node parameters for the OpenAI Chat Model node and links to more resources. Credentials You can find authentication information for this node here . Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Model # Select the model to use to generate the completion. n8n dynamically loads models from OpenAI and you'll only see the models available to your account. Node options # Use these options to further refine the node's behavior. Base URL # Enter a URL here to override the default URL for the API. Frequency Penalty # Use this option to control the chances of the model repeating itself. Higher values reduce the chance of the model repeating itself. Maximum Number of Tokens # Enter the maximum number of tokens used, which sets the completion length. Response Format # Choose Text or JSON . JSON ensures the model returns valid JSON. Presence Penalty # Use this option to control the chances of the model talking about new topics. Higher values increase the chance of the model talking about new topics. Sampling Temperature # Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations. Timeout # Enter the maximum request time in milliseconds. Max Retries # Enter the maximum number of times to retry a request. Top P # Use this option to set the probability the completion should use. Use a lower value to ignore less probable options. Templates and examples # AI agent chat by n8n Team View template details Scrape and summarize webpages with AI by n8n Team View template details AI agent that can scrape webpages by Eduard View template details Browse OpenAI Chat Model integration templates , or search all templates Related resources # Refer to LangChains's OpenAI documentation for more information about the service. View n8n's Advanced AI documentation. Common issues # For common questions or issues and suggested solutions, refer to Common issues . AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmcohere/

Cohere Model node # Use the Cohere Model node to use Cohere's models. On this page, you'll find the node parameters for the Cohere Model node, and links to more resources. Credentials You can find authentication information for this node here . Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node Options # Maximum Number of Tokens : Enter the maximum number of tokens used, which sets the completion length. Sampling Temperature : Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations. Templates and examples # AI agent chat by n8n Team View template details Scrape and summarize webpages with AI by n8n Team View template details AI agent that can scrape webpages by Eduard View template details Browse Cohere Model integration templates , or search all templates Related resources # Refer to LangChains's Cohere documentation for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmollama/

Ollama Model node # The Ollama Model node allows you use local Llama 2 models. On this page, you'll find the node parameters for the Ollama Model node, and links to more resources. Credentials You can find authentication information for this node here . Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Model : Select the model that generates the completion. Choose from: Llama2 Llama2 13B Llama2 70B Llama2 Uncensored Refer to the Ollama Models Library documentation for more information about available models. Node options # Sampling Temperature : Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations. Top K : Enter the number of token choices the model uses to generate the next token. Top P : Use this option to set the probability the completion should use. Use a lower value to ignore less probable options. Templates and examples # Chat with local LLMs using n8n and Ollama by Mihai Farcas View template details ðŸ”ðŸ¦™ðŸ¤– Private & Local Ollama Self-Hosted AI Assistant by Joseph LePage View template details ðŸ‹DeepSeek V3 Chat & R1 Reasoning Quick Start by Joseph LePage View template details Browse Ollama Model integration templates , or search all templates Related resources # Refer to LangChains's Ollama documentation for more information about the service. View n8n's Advanced AI documentation. Common issues # For common questions or issues and suggested solutions, refer to Common issues . AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Self-hosted AI Starter Kit # New to working with AI and using self-hosted n8n? Try n8n's self-hosted AI Starter Kit to get started with a proof-of-concept or demo playground using Ollama, Qdrant, and PostgreSQL. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmopenhuggingfaceinference/

Hugging Face Inference Model node # Use the Hugging Face Inference Model node to use Hugging Face's models. On this page, you'll find the node parameters for the Hugging Face Inference Model node, and links to more resources. Credentials You can find authentication information for this node here . Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Model : Select the model to use to generate the completion. Node options # Custom Inference Endpoint : Enter a custom inference endpoint URL. Frequency Penalty : Use this option to control the chances of the model repeating itself. Higher values reduce the chance of the model repeating itself. Maximum Number of Tokens : Enter the maximum number of tokens used, which sets the completion length. Presence Penalty : Use this option to control the chances of the model talking about new topics. Higher values increase the chance of the model talking about new topics. Sampling Temperature : Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations. Top K : Enter the number of token choices the model uses to generate the next token. Top P : Use this option to set the probability the completion should use. Use a lower value to ignore less probable options. Templates and examples # Browse Hugging Face Inference Model integration templates , or search all templates Related resources # Refer to LangChains's Hugging Face Inference Model documentation for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memorybufferwindow/

Window Buffer Memory node # Use the Window Buffer Memory node to persist chat history in your workflow. On this page, you'll find a list of operations the Window Buffer Memory node supports, and links to more resources. Don't use this node if running n8n in queue mode If your n8n instance uses queue mode , this node doesn't work in an active production workflow. This is because n8n can't guarantee that every call to Window Buffer Memory will go to the same worker. Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Configure these parameters to configure the node: Session Key : Enter the key to use to store the memory in the workflow data. Context Window Length : Enter the number of previous interactions to consider for context. Templates and examples # Browse Window Buffer Memory node documentation integration templates , or search all templates Related resources # Refer to LangChain's Buffer Window Memory documentation for more information about the service. View n8n's Advanced AI documentation. Common issues # For common questions or issues and suggested solutions, refer to Common issues . AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memorymanager/

Chat Memory Manager node # The Chat Memory Manager node manages chat message memories within your workflows. Use this node to load, insert, and delete chat messages in an in-memory vector store . This node is useful when you: Can't add a memory node directly. Need to do more complex memory management, beyond what the memory nodes offer. For example, you can add this node to check the memory size of the Agent node's response, and reduce it if needed. Want to inject messages to the AI that look like user messages, to give the AI more context. On this page, you'll find a list of operations that the Chat Memory Manager node supports, along with links to more resources. Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Operation Mode : Choose between Get Many Messages , Insert Messages , and Delete Messages operations. Insert Mode : Available in Insert Messages mode. Choose from: Insert Messages : Insert messages alongside existing messages. Override All Messages : Replace current memory. Delete Mode : available in Delete Messages mode. Choose from: Last N : Delete the last N messages. All Messages : Delete messages from memory. Chat Messages : available in Insert Messages mode. Define the chat messages to insert into the memory, including: Type Name or ID : Set the message type. Select one of: AI : Use this for messages from the AI. System : Add a message containing instructions for the AI. User : Use this for messages from the user. This message type is sometimes called the 'human' message in other AI tools and guides. Message : Enter the message contents. Hide Message in Chat : Select whether n8n should display the message to the user in the chat UI (turned off) or not (turned on). Messages Count : Available in Delete Messages mode when you select Last N . Enter the number of latest messages to delete. Simplify Output : Available in Get Many Messages mode. Turn on to simplify the output to include only the sender (AI, user, or system) and the text. Templates and examples # Chat with OpenAI Assistant (by adding a memory) by David Roberts View template details AI Voice Chat using Webhook, Memory Manager, OpenAI, Google Gemini & ElevenLabs by Ayoub View template details Conversational Interviews with AI Agents and n8n Forms by Jimleuk View template details Browse Chat Memory Manager integration templates , or search all templates Related resources # Refer to LangChain's Memory documentation for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memorymotorhead/

Motorhead node # Use the Motorhead node to use Motorhead as a memory server. On this page, you'll find a list of operations the Motorhead node supports, and links to more resources. Credentials You can find authentication information for this node here . Node parameters # Session ID : Enter the ID to use to store the memory in the workflow data. Node reference # Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Templates and examples # Browse Motorhead integration templates , or search all templates Related resources # Refer to LangChain's Motorhead documentation for more information about the service. View n8n's Advanced AI documentation. Single memory instance # If you add more than one Motorhead node to your workflow, all nodes access the same memory instance by default. Be careful when doing destructive actions that override existing memory contents, such as the override all messages operation in the Chat Memory Manager node. If you want more than one memory instance in your workflow, set different session IDs in different memory nodes. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memorypostgreschat/

Postgres Chat Memory node # Use the Postgres Chat Memory node to use Postgres as a memory server for storing chat history. On this page, you'll find a list of operations the Postgres Chat Memory node supports, and links to more resources. Credentials You can find authentication information for this node here . Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Session Key : Enter the key to use to store the memory in the workflow data. Table Name : Enter the name of the table to store the chat history in. The system will create the table if doesn't exist. Context Window Length : Enter the number of previous interactions to consider for context. Related resources # Refer to LangChain's Postgres Chat Message History documentation for more information about the service. View n8n's Advanced AI documentation. Single memory instance # If you add more than one Postgres Chat Memory node to your workflow, all nodes access the same memory instance by default. Be careful when doing destructive actions that override existing memory contents, such as the override all messages operation in the Chat Memory Manager node. If you want more than one memory instance in your workflow, set different session IDs in different memory nodes. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memoryredischat/

Redis Chat Memory node # Use the Redis Chat Memory node to use Redis as a memory server. On this page, you'll find a list of operations the Redis Chat Memory node supports, and links to more resources. Credentials You can find authentication information for this node here . Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Session Key : Enter the key to use to store the memory in the workflow data. Session Time To Live : Use this parameter to make the session expire after a given number of seconds. Context Window Length : Enter the number of previous interactions to consider for context. Templates and examples # Conversational Interviews with AI Agents and n8n Forms by Jimleuk View template details Enhance Customer Chat by Buffering Messages with Twilio and Redis by Jimleuk View template details ðŸ¶ AI Agent for PetShop Appointments (Agente de IA para agendamentos de PetShop) by Bruno Dias View template details Browse Redis Chat Memory integration templates , or search all templates Related resources # Refer to LangChain's Redis Chat Memory documentation for more information about the service. View n8n's Advanced AI documentation. Single memory instance # If you add more than one Redis Chat Memory node to your workflow, all nodes access the same memory instance by default. Be careful when doing destructive actions that override existing memory contents, such as the override all messages operation in the Chat Memory Manager node. If you want more than one memory instance in your workflow, set different session IDs in different memory nodes. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memoryxata/

Xata node # Use the Xata node to use Xata as a memory server. On this page, you'll find a list of operations the Xata node supports, and links to more resources. Credentials You can find authentication information for this node here . Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Session ID : Enter the ID to use to store the memory in the workflow data. Context Window Length : Enter the number of previous interactions to consider for context. Templates and examples # Scrape and summarize webpages with AI by n8n Team View template details Very quick quickstart by Deborah View template details Building Your First WhatsApp Chatbot by Jimleuk View template details Browse Xata integration templates , or search all templates Related resources # Refer to LangChain's Xata documentation for more information about the service. View n8n's Advanced AI documentation. Single memory instance # If you add more than one Xata node to your workflow, all nodes access the same memory instance by default. Be careful when doing destructive actions that override existing memory contents, such as the override all messages operation in the Chat Memory Manager node. If you want more than one memory instance in your workflow, set different session IDs in different memory nodes. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memoryzep/

Zep node # Use the Zep node to use Zep as a memory server. On this page, you'll find a list of operations the Zep node supports, and links to more resources. Credentials You can find authentication information for this node here . Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Session ID : Enter the ID to use to store the memory in the workflow data. Templates and examples # Browse Zep integration templates , or search all templates Related resources # Refer to LangChain's Zep documentation for more information about the service. View n8n's Advanced AI documentation. Single memory instance # If you add more than one Zep node to your workflow, all nodes access the same memory instance by default. Be careful when doing destructive actions that override existing memory contents, such as the override all messages operation in the Chat Memory Manager node. If you want more than one memory instance in your workflow, set different session IDs in different memory nodes. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.outputparserautofixing/

Auto-fixing Output Parser node # The Auto-fixing Output Parser node wraps another output parser. If the first one fails, it calls out to another LLM to fix any errors. Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Templates and examples # Notion AI Assistant Generator by Max Tkacz View template details Allow your AI to call an API to fetch data by Deborah View template details Handling Appointment Leads and Follow-up With Twilio, Cal.com and AI by Jimleuk View template details Browse Auto-fixing Output Parser integration templates , or search all templates Related resources # Refer to LangChain's output parser documentation for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.outputparseritemlist/

Item List Output Parser node # Use the Item List Output Parser node to return a list of items with a specific length and separator. Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node options # Number of Items : Enter the maximum items to return. Set to -1 for unlimited items. Separator : Select the separator used to split the results into separate items. Defaults to a new line. Templates and examples # Browse Item List Output Parser integration templates , or search all templates Related resources # Refer to LangChain's output parser documentation for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.outputparserstructured/

Structured Output Parser node # Use the Structured Output Parser node to return fields based on a JSON Schema. On this page, you'll find the node parameters for the Structured Output Parser node, and links to more resources. Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Schema Type : Define the output structure and validation. You have two options to provide the schema: Generate from JSON Example : Input an example JSON object to automatically generate the schema. The node uses the object property types and names. It ignores the actual values. Define Below : Manually input the JSON schema. Read the JSON Schema guides and examples for help creating a valid JSON schema. Templates and examples # Chat with PDF docs using AI (quoting sources) by David Roberts View template details Suggest meeting slots using AI by n8n Team View template details Autonomous AI crawler by Oskar View template details Browse Structured Output Parser integration templates , or search all templates Related resources # Refer to LangChain's output parser documentation for more information about the service. View n8n's Advanced AI documentation. Common issues # For common questions or issues and suggested solutions, refer to Common issues . AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.retrievercontextualcompression/

Contextual Compression Retriever node # The Contextual Compression Retriever node improves the answers returned from vector store document similarity searches by taking into account the context from the query. Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Templates and examples # Ask questions about a PDF using AI by David Roberts View template details AI Crew to Automate Fundamental Stock Analysis - Q&A Workflow by Derek Cheung View template details Advanced AI Demo (Presented at AI Developers #14 meetup) by Max Tkacz View template details Browse Contextual Compression Retriever integration templates , or search all templates Related resources # Refer to LangChain's contextual compression retriever documentation for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.retrievermultiquery/

MultiQuery Retriever node # The MultiQuery Retriever node automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. On this page, you'll find the node parameters for the MultiQuery Retriever node, and links to more resources. Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node options # Query Count : Enter how many different versions of the query to generate. Templates and examples # Ask questions about a PDF using AI by David Roberts View template details AI Crew to Automate Fundamental Stock Analysis - Q&A Workflow by Derek Cheung View template details Advanced AI Demo (Presented at AI Developers #14 meetup) by Max Tkacz View template details Browse MultiQuery Retriever integration templates , or search all templates Related resources # Refer to LangChain's retriever conceptual documentation and LangChain's multiquery retriever API documentation for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.retrievervectorstore/

Vector Store Retriever node # Use the Vector Store Retriever node to retrieve documents from a vector store . On this page, you'll find the node parameters for the Vector Store Retriever node, and links to more resources. Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Limit : Enter the maximum number of results to return. Templates and examples # Ask questions about a PDF using AI by David Roberts View template details AI Crew to Automate Fundamental Stock Analysis - Q&A Workflow by Derek Cheung View template details Advanced AI Demo (Presented at AI Developers #14 meetup) by Max Tkacz View template details Browse Vector Store Retriever integration templates , or search all templates Related resources # Refer to LangChain's vector store retriever documentation for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.retrieverworkflow/

Workflow Retriever node # Use the Workflow Retriever node to retrieve data from an n8n workflow for use in a Retrieval QA Chain or another Retriever node. On this page, you'll find the node parameters for the Workflow Retriever node, and links to more resources. Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Source # Tell n8n which workflow to call. You can choose either: Database and enter a workflow ID. Parameter and copy in a complete workflow JSON . Workflow values # Set values to pass to the workflow you're calling. These values appear in the output data of the trigger node in the workflow you call. You can access these values in expressions in the workflow. For example, if you have: Workflow Values with a Name of myCustomValue A workflow with an Execute Sub-workflow Trigger node as its trigger The expression to access the value of myCustomValue is {{ $('Execute Sub-workflow Trigger').item.json.myCustomValue }} . Templates and examples # AI Crew to Automate Fundamental Stock Analysis - Q&A Workflow by Derek Cheung View template details AI: Ask questions about any data source (using the n8n workflow retriever) by n8n Team View template details âš¡Auto Workflow Positioning ! by Lucas Peyrin View template details Browse Workflow Retriever integration templates , or search all templates Related resources # Refer to LangChain's general retriever documentation for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.sentimentanalysis/

Sentiment Analysis node # Use the Sentiment Analysis node to analyze the sentiment of incoming text data. The language model uses the Sentiment Categories in the node options to determine each item's sentiment. Node parameters # Text to Analyze defines the input text for sentiment analysis. This is an expression that references a field from the input items. For example, this could be {{ $json.chatInput }} if the input is from a chat or message source. By default, it expects a text field. Node options # Sentiment Categories : Define the categories that you want to classify your input as. By default, these are Positive, Neutral, Negative . You can customize these categories to fit your specific use case, such as Very Positive, Positive, Neutral, Negative, Very Negative for more granular analysis. Include Detailed Results : When turned on, this option includes sentiment strength and confidence scores in the output. Note that these scores are estimates generated by the language model and are rough indicators rather than precise measurements. System Prompt Template : Use this option to change the system prompt that's used for the sentiment analysis. It uses the {categories} placeholder for the categories. Enable Auto-Fixing : When enabled, the node automatically fixes model outputs to ensure they match the expected format. Do this by sending the schema parsing error to the LLM and asking it to fix it. Usage Notes # Model Temperature Setting # It's strongly advised to set the temperature of the connected language model to 0 or a value close to 0. This helps ensure that the results are as deterministic as possible, providing more consistent and reliable sentiment analysis across multiple runs. Language Considerations # The node's performance may vary depending on the language of the input text. For best results, ensure your chosen language model supports the input language. Processing Large Volumes # When analyzing large amounts of text, consider splitting the input into smaller chunks to optimize processing time and resource usage. Iterative Refinement # For complex sentiment analysis tasks, you may need to iteratively refine the system prompt and categories to achieve the desired results. Example Usage # Basic Sentiment Analysis # Connect a data source (for example, RSS Feed, HTTP Request) to the Sentiment Analysis node. Set the "Text to Analyze" field to the relevant item property (for example, {{ $json.content }} for blog post content). Keep the default sentiment categories. Connect the node's outputs to separate paths for processing positive, neutral, and negative sentiments differently. Custom Category Analysis # Change the Sentiment Categories to Excited, Happy, Neutral, Disappointed, Angry . Adjust your workflow to handle these five output categories. Use this setup to analyze customer feedback with more nuanced emotional categories. Related resources # View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.text-classifier/

Text Classifier node # Use the Text Classifier node to classify (categorize) incoming data. Using the categories provided in the parameters (see below), each item is passed to the model to determine its category. On this page, you'll find the node parameters for the Text Classifier node, and links to more resources. Node parameters # Input Prompt defines the input to classify. This is usually an expression that references a field from the input items. For example, this could be {{ $json.chatInput }} if the input is a chat trigger. By default it references the text field. Categories : Add the categories that you want to classify your input as. Categories have a name and a description. Use the description to tell the model what the category means. This is important if the meaning isn't obvious. You can add as many categories as you like. Node options # Allow Multiple Classes To Be True : You can configure the classifier to always output a single class per item (turned off), or allow the model to select multiple classes (turned on). When No Clear Match : Define what happens if the model can't find a good match for an item. There are two options: Discard Item (the default): If the node doesn't detect any of the categories, it drops the item. Output on Extra, 'Other' Branch : Creates a separate output branch called Other . When the node doesn't detect any of the categories, it outputs items in this branch. System Prompt Template : Use this option to change the system prompt that's used for the classification. It uses the {categories} placeholder for the categories. Enable Auto-Fixing : When enabled, the node automatically fixes model outputs to ensure they match the expected format. Do this by sending the schema parsing error to the LLM and asking it to fix it. Related resources # View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.textsplittercharactertextsplitter/

Character Text Splitter node # Use the Character Text Splitter node to split document data based on characters. On this page, you'll find the node parameters for the Character Text Splitter node, and links to more resources. Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Separator : Select the separator used to split the document into separate items. Chunk Size : Enter the number of characters in each chunk. Chunk Overlap : Enter how much overlap to have between chunks. Templates and examples # Scrape and summarize webpages with AI by n8n Team View template details Building Your First WhatsApp Chatbot by Jimleuk View template details Ask questions about a PDF using AI by David Roberts View template details Browse Character Text Splitter integration templates , or search all templates Related resources # Refer to LangChain's text splitter documentation and LangChain's API documentation for character text splitting for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.textsplitterrecursivecharactertextsplitter/

Recursive Character Text Splitter node # The Recursive Character Text Splitter node splits document data recursively to keep all paragraphs, sentences then words together as long as possible. On this page, you'll find the node parameters for the Recursive Character Text Splitter node, and links to more resources. Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Chunk Size : Enter the number of characters in each chunk. Chunk Overlap : Enter how much overlap to have between chunks. Templates and examples # Scrape and summarize webpages with AI by n8n Team View template details Building Your First WhatsApp Chatbot by Jimleuk View template details Ask questions about a PDF using AI by David Roberts View template details Browse Recursive Character Text Splitter integration templates , or search all templates Related resources # Refer to LangChain's text splitter documentation and LangChain's recursively split by character documentation for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.textsplittertokensplitter/

Token Splitter node # The Token Splitter node splits a raw text string by first converting the text into BPE tokens, then splits these tokens into chunks and converts the tokens within a single chunk back into text. On this page, you'll find the node parameters for the Token Splitter node, and links to more resources. Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Chunk Size : Enter the number of characters in each chunk. Chunk Overlap : Enter how much overlap to have between chunks. Templates and examples # Building RAG Chatbot for Movie Recommendations with Qdrant and Open AI by Jenny View template details AI Voice Chatbot with ElevenLabs & OpenAI for Customer Service and Restaurants by n3w Italia View template details Load and summarize Google Drive files with AI by n8n Team View template details Browse Token Splitter integration templates , or search all templates Related resources # Refer to LangChain's token documentation and LangChain's text splitter documentation for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolcalculator/

Calculator node # The Calculator node is a tool that allows an agent to run mathematical calculations. Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Templates and examples # Chat with OpenAI Assistant (by adding a memory) by David Roberts View template details AI marketing report (Google Analytics & Ads, Meta Ads), sent via email/Telegram by Friedemann Schuetz View template details ERP AI chatbot for Odoo sales module with OpenAI by Mihai Farcas View template details Browse Calculator integration templates , or search all templates Related resources # Refer to LangChain's documentation on tools for more information about tools in LangChain. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolcode/

Custom Code Tool node # Use the Custom Code Tool node to write code that an agent can run. On this page, you'll find the node parameters for the Custom Code Tool node and links to more resources. Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Name # Give your custom code a name. It can't contain whitespace. Description # Give your custom code a description. This tells the agent when to use this tool. For example: Call this tool to get a random color. The input should be a string with comma separated names of colors to exclude. Language # You can use JavaScript or Python. JavaScript / Python box # Write the code here. You can access the tool input using query . For example, to take the input string and lowercase it: 1 2 let myString = query ; return myString . toLowerCase (); Templates and examples # Browse Custom Code Tool integration templates , or search all templates Related resources # Refer to LangChain's documentation on tools for more information about tools in LangChain. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolhttprequest/

HTTP Request Tool node # The HTTP Request tool works just like the HTTP Request node, but it's designed to be used with an AI agent as a tool to collect information from a website or API. On this page, you'll find a list of operations the HTTP Request node supports and links to more resources. Credentials Refer to HTTP Request credentials for guidance on setting up authentication. Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Templates and examples # AI agent that can scrape webpages by Eduard View template details Building Your First WhatsApp Chatbot by Jimleuk View template details Autonomous AI crawler by Oskar View template details Browse HTTP Request Tool integration templates , or search all templates Related resources # Refer to LangChain's documentation on tools for more information about tools in LangChain. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolserpapi/

SerpApi (Google Search) node # The SerpAPI node allows an agent in your workflow to call Google's Search API. On this page, you'll find the node parameters for the SerpAPI node, and links to more resources. Credentials You can find authentication information for this node here . Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node options # Country : Enter the country code you'd like to use. Refer to Google GL Parameter: Supported Google Countries for supported countries and country codes. Device : Select the device to use to get the search results. Explicit Array : Choose whether to force SerpApi to fetch the Google results even if a cached version is already present (turned on) or not (turned off). Google Domain : Enter the Google Domain to use. Refer to Supported Google Domains for supported domains. Language : Enter the language code you'd like to use. Refer to Google HL Parameter: Supported Google Languages for supported languages and language codes. Templates and examples # AI agent chat by n8n Team View template details AI chatbot that can search the web by n8n Team View template details AI web researcher for sales by Lucas Perret View template details Browse SerpApi (Google Search) integration templates , or search all templates Related resources # Refer to Serp's documentation for more information about the service. You can also view LangChain's documentation on their Serp integration . View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolvectorstore/

Vector Store Question Answer Tool node # The Vector Store Question Answer node is a tool that allows an agent to summarize results and answer questions based on chunks from a vector store . On this page, you'll find the node parameters for the Vector Store Question Answer node, and links to more resources. Examples and templates For usage examples and templates to help you get started, refer to n8n's Vector Store Question Answer Tool integrations page. Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Data Name # Enter the name of the data in the vector store. Description of Data # Enter a description of the data in the vector store. n8n uses the Data Name and Description of Data parameters to populate the tool description for AI agents using the following format: Useful for when you need to answer questions about [Data Name]. Whenever you need information about [Description of Data], you should ALWAYS use this. Input should be a fully formed question. Limit # The maximum number of results to return. Related resources # View example workflows and related content on n8n's website. Refer to LangChain's documentation on tools for more information about tools in LangChain. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolwikipedia/

Wikipedia node # The Wikipedia node is a tool that allows an agent to search and return information from Wikipedia. Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Templates and examples # Write a WordPress post with AI (starting from a few keywords) by Giulio View template details AI chatbot that can search the web by n8n Team View template details Respond to WhatsApp Messages with AI Like a Pro! by Jimleuk View template details Browse Wikipedia integration templates , or search all templates Related resources # Refer to LangChain's documentation on tools for more information about tools in LangChain. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolwolframalpha/

Wolfram|Alpha tool node # Use the Wolfram|Alpha tool node to connect your agents and chains to Wolfram|Alpha's computational intelligence engine. Credentials You can find authentication information for this node here . Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Templates and examples # Browse Wolfram|Alpha integration templates , or search all templates Related resources # Refer to Wolfram|Alpha's documentation for more information about the service. You can also view LangChain's documentation on their WolframAlpha Tool . View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolworkflow/

Custom n8n Workflow Tool node # The Workflow Tool node is a tool that allows an agent to run another n8n workflow and fetch its output data. On this page, you'll find the node parameters for the Workflow Tool node, and links to more resources. Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node parameters # Name # Enter a name for your custom code. It can't contain whitespace or special characters. Description # Enter a custom code a description. This tells the agent when to use this tool. For example: Call this tool to get a random color. The input should be a string with comma separated names of colors to exclude. Source # Tell n8n which workflow to call. You can choose either: Database and enter a workflow ID. Parameter and copy in a complete workflow JSON . Workflow Values # Set values to pass to the workflow you're calling. These values appear in the output data of the trigger node in the workflow you call. You can access these values in expressions in the workflow. For example, if you have: Workflow Values with a Name of myCustomValue A workflow with an Execute Sub-workflow Trigger node as its trigger The expression to access the value of myCustomValue is {{ $('Execute Sub-workflow Trigger').item.json.myCustomValue }} . Specify input schema # Agent support The structured input schema requires with a Tools Agent or OpenAI Functions Agent. Enable this option to define the input schema for the workflow you're calling. This is useful when you want to make sure the input data the LLM provides is in the correct format. Schema Type : Define the input structure and validation. You have two options to provide the schema: Generate from JSON Example : Input an example JSON object to automatically generate the schema. The node uses the object property types and names. It ignores the actual values. Define Below : Manually input the JSON schema. Read the JSON Schema guides and examples for help creating a valid JSON schema. Templates and examples # Browse Custom n8n Workflow Tool integration templates , or search all templates Related resources # Refer to LangChain's documentation on tools for more information about tools in LangChain. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstoreinmemory/

In-Memory Vector Store node # Use the In Memory Vector Store node to store and retrieve embeddings in n8n's in-app memory. On this page, you'll find the node parameters for the In Memory Vector Store node, and links to more resources. Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. This node is different to AI memory nodes The in-memory storage described here is different to the AI memory nodes such as Window Buffer Memory . This node creates a vector database in the app memory. Node usage patterns # You can use the In-Memory Vector Store node in the following patterns. Use as a regular node to insert and retrieve documents # You can use the In-Memory Vector Store as a regular node to insert or get documents. This pattern places the In-Memory Vector Store in the regular connection flow without using an agent. You can see an example of in step 2 of this template . Connect directly to an AI agent as a tool # You can connect the In-Memory Vector Store node directly to the tool connector of an AI agent to use vector store as a resource when answering queries. Here, the connection would be: AI agent (tools connector) -> In-Memory Vector Store node. Use a retriever to fetch documents # You can use the Vector Store Retriever node with the In-Memory Vector Store node to fetch documents from the In-Memory Vector Store node. This is often used with the Question and Answer Chain node to fetch documents from the vector store that match the given chat input. An example of the connection flow (the linked example uses Pinecone, but the pattern is the same) would be: Question and Answer Chain (Retriever connector) -> Vector Store Retriever (Vector Store connector) -> In-Memory Vector Store. Use the Vector Store Question Answer Tool to answer questions # Another pattern uses the Vector Store Question Answer Tool to summarize results and answer questions from the In-Memory Vector Store node. Rather than connecting the In-Memory Vector Store directly as a tool, this pattern uses a tool specifically designed to summarizes data in the vector store. The connections flow in this case would look like this: AI agent (tools connector) -> Vector Store Question Answer Tool (Vector Store connector) -> In-Memory Vector store. Node parameters # Operation Mode # This Vector Store node has four modes: Get Many , Insert Documents , Retrieve Documents (As Vector Store for Chain/Tool) , and Retrieve Documents (As Tool for AI Agent) . The mode you select determines the operations you can perform with the node and what inputs and outputs are available. Get Many # In this mode, you can retrieve multiple documents from your vector database by providing a prompt. The prompt will be embedded and used for similarity search. The node will return the documents that are most similar to the prompt with their similarity score. This is useful if you want to retrieve a list of similar documents and pass them to an agent as additional context. Insert Documents # Use Insert Documents mode to insert new documents into your vector database. Retrieve Documents (As Vector Store for Chain/Tool) # Use Retrieve Documents (As Vector Store for Chain/Tool) mode with a vector-store retriever to retrieve documents from a vector database and provide them to the retriever connected to a chain. In this mode you must connect the node to a retriever node or root node. Retrieve Documents (As Tool for AI Agent) # Use Retrieve Documents (As Tool for AI Agent) mode to use the vector store as a tool resource when answering queries. When formulating responses, the agent uses the vector store when the vector store name and description match the question details. Get Many parameters # Memory Key : Enter the key to use to store the vector memory in the workflow data. n8n prefixes the key with the workflow ID to avoid collisions. Prompt : Enter the search query. Limit : Enter how many results to retrieve from the vector store. For example, set this to 10 to get the ten best results. Insert Documents parameters # Memory Key : Enter the key to use to store the vector memory in the workflow data. n8n prefixes the key with the workflow ID to avoid collisions. Clear Store : Use this parameter to control whether to wipe the vector store for the given memory key for this workflow before inserting data (turned on). Retrieve Documents (As Vector Store for Chain/Tool) parameters # Memory Key : Enter the key to use to store the vector memory in the workflow data. n8n prefixes the key with the workflow ID to avoid collisions. Retrieve Documents (As Tool for AI Agent) parameters # Name : The name of the vector store. Description : Explain to the LLM what this tool does. A good, specific description allows LLMs to produce expected results more often. Memory Key : Enter the key to use to store the vector memory in the workflow data. n8n prefixes the key with the workflow ID to avoid collisions. Limit : Enter how many results to retrieve from the vector store. For example, set this to 10 to get the ten best results. Templates and examples # Building Your First WhatsApp Chatbot by Jimleuk View template details Generating Image Embeddings via Textual Summarisation by Jimleuk View template details âš¡Auto Workflow Positioning ! by Lucas Peyrin View template details Browse In-Memory Vector Store integration templates , or search all templates Related resources # Refer to LangChains's Memory Vector Store documentation for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstorepgvector/

PGVector Vector Store node # PGVector is an extension of Postgresql. Use this node to interact with the PGVector tables in your Postgresql database. You can insert documents into a vector table, get documents from a vector table, retrieve documents to provide them to a retriever connected to a chain , or connect directly to an agent as a tool . On this page, you'll find the node parameters for the PGVector node, and links to more resources. Credentials You can find authentication information for this node here . Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node usage patterns # You can use the PGVector Vector Store node in the following patterns. Use as a regular node to insert and retrieve documents # You can use the PGVector Vector Store as a regular node to insert or get documents. This pattern places the PGVector Vector Store in the regular connection flow without using an agent. You can see an example of this in scenario 1 of this template (the template uses the Supabase Vector Store, but the pattern is the same). Connect directly to an AI agent as a tool # You can connect the PGVector Vector Store node directly to the tool connector of an AI agent to use vector store as a resource when answering queries. Here, the connection would be: AI agent (tools connector) -> PGVector Vector Store node. Use a retriever to fetch documents # You can use the Vector Store Retriever node with the PGVector Vector Store node to fetch documents from the PGVector Vector Store node. This is often used with the Question and Answer Chain node to fetch documents from the vector store that match the given chat input. An example of the connection flow (the linked example uses Pinecone, but the pattern is the same) would be: Question and Answer Chain (Retriever connector) -> Vector Store Retriever (Vector Store connector) -> PGVector Vector Store. Use the Vector Store Question Answer Tool to answer questions # Another pattern uses the Vector Store Question Answer Tool to summarize results and answer questions from the PGVector Vector Store node. Rather than connecting the PGVector Vector Store directly as a tool, this pattern uses a tool specifically designed to summarizes data in the vector store. The connections flow (the linked example uses the In-Memory Vector Store, but the pattern is the same) in this case would look like this: AI agent (tools connector) -> Vector Store Question Answer Tool (Vector Store connector) -> In-Memory Vector store. Node parameters # Operation Mode # This Vector Store node has four modes: Get Many , Insert Documents , Retrieve Documents (As Vector Store for Chain/Tool) , and Retrieve Documents (As Tool for AI Agent) . The mode you select determines the operations you can perform with the node and what inputs and outputs are available. Get Many # In this mode, you can retrieve multiple documents from your vector database by providing a prompt. The prompt will be embedded and used for similarity search. The node will return the documents that are most similar to the prompt with their similarity score. This is useful if you want to retrieve a list of similar documents and pass them to an agent as additional context. Insert Documents # Use Insert Documents mode to insert new documents into your vector database. Retrieve Documents (As Vector Store for Chain/Tool) # Use Retrieve Documents (As Vector Store for Chain/Tool) mode with a vector-store retriever to retrieve documents from a vector database and provide them to the retriever connected to a chain. In this mode you must connect the node to a retriever node or root node. Retrieve Documents (As Tool for AI Agent) # Use Retrieve Documents (As Tool for AI Agent) mode to use the vector store as a tool resource when answering queries. When formulating responses, the agent uses the vector store when the vector store name and description match the question details. Get Many parameters # Table name : Enter the name of the table you want to query. Prompt : Enter your search query. Limit : Enter a number to set how many results to retrieve from the vector store. For example, set this to 10 to get the ten best results. Insert Documents parameters # Table name : Enter the name of the table you want to query. Retrieve Documents parameters (As Vector Store for Chain/Tool) # Table name : Enter the name of the table you want to query. Retrieve Documents (As Tool for AI Agent) parameters # Name : The name of the vector store. Description : Explain to the LLM what this tool does. A good, specific description allows LLMs to produce expected results more often. Table Name : Enter the PGVector table to use. Limit : Enter how many results to retrieve from the vector store. For example, set this to 10 to get the ten best results. Node options # Collection # A way to separate datasets in PGVector. This creates a separate table and column to keep track of which collection a vector belongs to. Use Collection : Select whether to use a collection (turned on) or not (turned off). Collection Name : Enter the name of the collection you want to use. Collection Table Name : Enter the name of the table to store collection information in. Column Names # The following options specify the names of the columns to store the vectors and corresponding information in: ID Column Name Vector Column Name Content Column Name Metadata Column Name Metadata Filter # Available in Get Many mode. When searching for data, use this to match with metadata associated with the document. This is an AND query. If you specify more than one metadata filter field, all of them must match. When inserting data, the metadata is set using the document loader. Refer to Default Data Loader for more information on loading documents. Templates and examples # Browse PGVector Vector Store integration templates , or search all templates Related resources # Refer to LangChain's PGVector documentation for more information about the service. View n8n's Advanced AI documentation. Self-hosted AI Starter Kit # New to working with AI and using self-hosted n8n? Try n8n's self-hosted AI Starter Kit to get started with a proof-of-concept or demo playground using Ollama, Qdrant, and PostgreSQL. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstorepinecone/

Pinecone Vector Store node # Use the Pinecone node to interact with your Pinecone database as vector store . You can insert documents into a vector database, get documents from a vector database, retrieve documents to provide them to a retriever connected to a chain , or connect directly to an agent as a tool . On this page, you'll find the node parameters for the Pinecone node, and links to more resources. Credentials You can find authentication information for this node here . Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node usage patterns # You can use the Pinecone Vector Store node in the following patterns. Use as a regular node to insert, update, and retrieve documents # You can use the Pinecone Vector Store as a regular node to insert, update, or get documents. This pattern places the Pinecone Vector Store in the regular connection flow without using an agent. You can see an example of this in scenario 1 of this template . Connect directly to an AI agent as a tool # You can connect the Pinecone Vector Store node directly to the tool connector of an AI agent to use vector store as a resource when answering queries. Here, the connection would be: AI agent (tools connector) -> Pinecone Vector Store node. Use a retriever to fetch documents # You can use the Vector Store Retriever node with the Pinecone Vector Store node to fetch documents from the Pinecone Vector Store node. This is often used with the Question and Answer Chain node to fetch documents from the vector store that match the given chat input. An example of the connection flow would be: Question and Answer Chain (Retriever connector) -> Vector Store Retriever (Vector Store connector) -> Pinecone Vector Store. Use the Vector Store Question Answer Tool to answer questions # Another pattern uses the Vector Store Question Answer Tool to summarize results and answer questions from the Pinecone Vector Store node. Rather than connecting the Pinecone Vector Store directly as a tool, this pattern uses a tool specifically designed to summarizes data in the vector store. The connections flow in this case would look like this: AI agent (tools connector) -> Vector Store Question Answer Tool (Vector Store connector) -> Pinecone Vector store. Node parameters # Operation Mode # This Vector Store node has five modes: Get Many , Insert Documents , Retrieve Documents (As Vector Store for Chain/Tool) , Retrieve Documents (As Tool for AI Agent) , and Update Documents . The mode you select determines the operations you can perform with the node and what inputs and outputs are available. Get Many # In this mode, you can retrieve multiple documents from your vector database by providing a prompt. The prompt will be embedded and used for similarity search. The node will return the documents that are most similar to the prompt with their similarity score. This is useful if you want to retrieve a list of similar documents and pass them to an agent as additional context. Insert Documents # Use Insert Documents mode to insert new documents into your vector database. Retrieve Documents (As Vector Store for Chain/Tool) # Use Retrieve Documents (As Vector Store for Chain/Tool) mode with a vector-store retriever to retrieve documents from a vector database and provide them to the retriever connected to a chain. In this mode you must connect the node to a retriever node or root node. Retrieve Documents (As Tool for AI Agent) # Use Retrieve Documents (As Tool for AI Agent) mode to use the vector store as a tool resource when answering queries. When formulating responses, the agent uses the vector store when the vector store name and description match the question details. Update Documents # Use Update Documents mode to update documents in a vector database by ID. Fill in the ID with the ID of the embedding entry to update. Get Many parameters # Pinecone Index : Select or enter the Pinecone Index to use. Prompt : Enter your search query. Limit : Enter how many results to retrieve from the vector store. For example, set this to 10 to get the ten best results. Insert Documents parameters # Pinecone Index : Select or enter the Pinecone Index to use. Retrieve Documents (As Vector Store for Chain/Tool) parameters # Pinecone Index : Select or enter the Pinecone Index to use. Retrieve Documents (As Tool for AI Agent) parameters # Name : The name of the vector store. Description : Explain to the LLM what this tool does. A good, specific description allows LLMs to produce expected results more often. Pinecone Index : Select or enter the Pinecone Index to use. Limit : Enter how many results to retrieve from the vector store. For example, set this to 10 to get the ten best results. Node options # Pinecone Namespace # Another segregation option for how to store your data within the index. Metadata Filter # Available in Get Many mode. When searching for data, use this to match with metadata associated with the document. This is an AND query. If you specify more than one metadata filter field, all of them must match. When inserting data, the metadata is set using the document loader. Refer to Default Data Loader for more information on loading documents. Clear Namespace # Available in Insert Documents mode. Deletes all data from the namespace before inserting the new data. Templates and examples # Ask questions about a PDF using AI by David Roberts View template details Chat with PDF docs using AI (quoting sources) by David Roberts View template details RAG Chatbot for Company Documents using Google Drive and Gemini by Mihai Farcas View template details Browse Pinecone Vector Store integration templates , or search all templates Related resources # Refer to LangChain's Pinecone documentation for more information about the service. View n8n's Advanced AI documentation. Find your Pinecone index and namespace # Your Pinecone index and namespace are available in your Pinecone account. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstoreqdrant/

Qdrant Vector Store node # Use the Qdrant node to interact with your Qdrant collection as a vector store . You can insert documents into a vector database, get documents from a vector database, retrieve documents to provide them to a retriever connected to a chain or connect it directly to an agent to use as a tool . On this page, you'll find the node parameters for the Qdrant node, and links to more resources. Credentials You can find authentication information for this node here . Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node usage patterns # You can use the Qdrant Vector Store node in the following patterns. Use as a regular node to insert and retrieve documents # You can use the Qdrant Vector Store as a regular node to insert or get documents. This pattern places the Qdrant Vector Store in the regular connection flow without using an agent. You can see an example of this in the first part of this template . Connect directly to an AI agent as a tool # You can connect the Qdrant Vector Store node directly to the tool connector of an AI agent to use vector store as a resource when answering queries. Here, the connection would be: AI agent (tools connector) -> Qdrant Vector Store node. Use a retriever to fetch documents # You can use the Vector Store Retriever node with the Qdrant Vector Store node to fetch documents from the Qdrant Vector Store node. This is often used with the Question and Answer Chain node to fetch documents from the vector store that match the given chat input. An example of the connection flow would be: Question and Answer Chain (Retriever connector) -> Vector Store Retriever (Vector Store connector) -> Qdrant Vector Store. Use the Vector Store Question Answer Tool to answer questions # Another pattern uses the Vector Store Question Answer Tool to summarize results and answer questions from the Qdrant Vector Store node. Rather than connecting the Qdrant Vector Store directly as a tool, this pattern uses a tool specifically designed to summarizes data in the vector store. The connections flow in this case would look like this: AI agent (tools connector) -> Vector Store Question Answer Tool (Vector Store connector) -> Qdrant Vector store. Node parameters # Operation Mode # This Vector Store node has four modes: Get Many , Insert Documents , Retrieve Documents (As Vector Store for Chain/Tool) , and Retrieve Documents (As Tool for AI Agent) . The mode you select determines the operations you can perform with the node and what inputs and outputs are available. Get Many # In this mode, you can retrieve multiple documents from your vector database by providing a prompt. The prompt will be embedded and used for similarity search. The node will return the documents that are most similar to the prompt with their similarity score. This is useful if you want to retrieve a list of similar documents and pass them to an agent as additional context. Insert Documents # Use Insert Documents mode to insert new documents into your vector database. Retrieve Documents (As Vector Store for Chain/Tool) # Use Retrieve Documents (As Vector Store for Chain/Tool) mode with a vector-store retriever to retrieve documents from a vector database and provide them to the retriever connected to a chain. In this mode you must connect the node to a retriever node or root node. Retrieve Documents (As Tool for AI Agent) # Use Retrieve Documents (As Tool for AI Agent) mode to use the vector store as a tool resource when answering queries. When formulating responses, the agent uses the vector store when the vector store name and description match the question details. Get Many parameters # Qdrant collection name : Enter the name of the Qdrant collection to use. Prompt : Enter the search query. Limit : Enter how many results to retrieve from the vector store. For example, set this to 10 to get the ten best results. This Operation Mode includes one Node option , the Metadata Filter . Insert Documents parameters # Qdrant collection name : Enter the name of the Qdrant collection to use. This Operation Mode includes one Node option : Collection Config : Enter JSON options for creating a Qdrant collection creation configuration. Refer to the Qdrant Collections documentation for more information. Retrieve Documents (As Vector Store for Chain/Tool) parameters # Qdrant Collection : Enter the name of the Qdrant collection to use. This Operation Mode includes one Node option , the Metadata Filter . Retrieve Documents (As Tool for AI Agent) parameters # Name : The name of the vector store. Description : Explain to the LLM what this tool does. A good, specific description allows LLMs to produce expected results more often. Qdrant Collection : Enter the name of the Qdrant collection to use. Limit : Enter how many results to retrieve from the vector store. For example, set this to 10 to get the ten best results. Node options # Metadata Filter # Available in Get Many mode. When searching for data, use this to match with metadata associated with the document. This is an AND query. If you specify more than one metadata filter field, all of them must match. When inserting data, the metadata is set using the document loader. Refer to Default Data Loader for more information on loading documents. Templates and examples # Building RAG Chatbot for Movie Recommendations with Qdrant and Open AI by Jenny View template details AI Voice Chatbot with ElevenLabs & OpenAI for Customer Service and Restaurants by n3w Italia View template details AI Crew to Automate Fundamental Stock Analysis - Q&A Workflow by Derek Cheung View template details Browse Qdrant Vector Store integration templates , or search all templates Related resources # Refer to LangChain's Qdrant documentation for more information about the service. View n8n's Advanced AI documentation. Self-hosted AI Starter Kit # New to working with AI and using self-hosted n8n? Try n8n's self-hosted AI Starter Kit to get started with a proof-of-concept or demo playground using Ollama, Qdrant, and PostgreSQL. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstoresupabase/

Supabase Vector Store node # Use the Supabase Vector Store to interact with your Supabase database as vector store . You can insert documents into a vector database, get documents from a vector database, retrieve documents to provide them to a retriever connected to a chain , or connect it directly to an agent to use as a tool . On this page, you'll find the node parameters for the Supabase node, and links to more resources. Credentials You can find authentication information for this node here . Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Supabase provides a quickstart for setting up your vector store . If you use settings other than the defaults in the quickstart, this may affect parameter settings in n8n. Make sure you understand what you're doing. Node usage patterns # You can use the Supabase Vector Store node in the following patterns. Use as a regular node to insert, update, and retrieve documents # You can use the Supabase Vector Store as a regular node to insert, update, or get documents. This pattern places the Supabase Vector Store in the regular connection flow without using an agent. You can see an example of this in scenario 1 of this template . Connect directly to an AI agent as a tool # You can connect the Supabase Vector Store node directly to the tool connector of an AI agent to use vector store as a resource when answering queries. Here, the connection would be: AI agent (tools connector) -> Supabase Vector Store node. Use a retriever to fetch documents # You can use the Vector Store Retriever node with the Supabase Vector Store node to fetch documents from the Supabase Vector Store node. This is often used with the Question and Answer Chain node to fetch documents from the vector store that match the given chat input. An example of the connection flow (the example uses Pinecone, but the pattern in the same) would be: Question and Answer Chain (Retriever connector) -> Vector Store Retriever (Vector Store connector) -> Supabase Vector Store. Use the Vector Store Question Answer Tool to answer questions # Another pattern uses the Vector Store Question Answer Tool to summarize results and answer questions from the Supabase Vector Store node. Rather than connecting the Supabase Vector Store directly as a tool, this pattern uses a tool specifically designed to summarizes data in the vector store. The connections flow in this case would look like this: AI agent (tools connector) -> Vector Store Question Answer Tool (Vector Store connector) -> Supabase Vector store. Node parameters # Operation Mode # This Vector Store node has five modes: Get Many , Insert Documents , Retrieve Documents (As Vector Store for Chain/Tool) , Retrieve Documents (As Tool for AI Agent) , and Update Documents . The mode you select determines the operations you can perform with the node and what inputs and outputs are available. Get Many # In this mode, you can retrieve multiple documents from your vector database by providing a prompt. The prompt will be embedded and used for similarity search. The node will return the documents that are most similar to the prompt with their similarity score. This is useful if you want to retrieve a list of similar documents and pass them to an agent as additional context. Insert Documents # Use Insert Documents mode to insert new documents into your vector database. Retrieve Documents (As Vector Store for Chain/Tool) # Use Retrieve Documents (As Vector Store for Chain/Tool) mode with a vector-store retriever to retrieve documents from a vector database and provide them to the retriever connected to a chain. In this mode you must connect the node to a retriever node or root node. Retrieve Documents (As Tool for AI Agent) # Use Retrieve Documents (As Tool for AI Agent) mode to use the vector store as a tool resource when answering queries. When formulating responses, the agent uses the vector store when the vector store name and description match the question details. Update Documents # Use Update Documents mode to update documents in a vector database by ID. Fill in the ID with the ID of the embedding entry to update. Get Many parameters # Table Name : Enter the Supabase table to use. Prompt : Enter the search query. Limit : Enter how many results to retrieve from the vector store. For example, set this to 10 to get the ten best results. Insert Documents parameters # Table Name : Enter the Supabase table to use. Retrieve Documents (As Vector Store for Chain/Tool) parameters # Table Name : Enter the Supabase table to use. Retrieve Documents (As Tool for AI Agent) parameters # Name : The name of the vector store. Description : Explain to the LLM what this tool does. A good, specific description allows LLMs to produce expected results more often. Table Name : Enter the Supabase table to use. Limit : Enter how many results to retrieve from the vector store. For example, set this to 10 to get the ten best results. Update Documents # Table Name : Enter the Supabase table to use. ID : The ID of an embedding entry. Node options # Query Name # The name of the matching function you set up in Supabase. If you follow the Supabase quickstart , this will be match_documents . Metadata Filter # Available in Get Many mode. When searching for data, use this to match with metadata associated with the document. This is an AND query. If you specify more than one metadata filter field, all of them must match. When inserting data, the metadata is set using the document loader. Refer to Default Data Loader for more information on loading documents. Templates and examples # AI Agent To Chat With Files In Supabase Storage by Mark Shcherbakov View template details Supabase Insertion & Upsertion & Retrieval by Ria View template details Upsert huge documents in a vector store with Supabase and Notion by Mario View template details Browse Supabase Vector Store integration templates , or search all templates Related resources # Refer to LangChain's Supabase documentation for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstorezep/

Zep Vector Store node # Use the Zep Vector Store to interact with Zep vector databases. You can insert documents into a vector database, get documents from a vector database, retrieve documents to provide them to a retriever connected to a chain , or connect it directly to an agent to use as a tool . On this page, you'll find the node parameters for the Zep Vector Store node, and links to more resources. Credentials You can find authentication information for this node here . Examples and templates For usage examples and templates to help you get started, refer to n8n's Zep Vector Store integrations page. Parameter resolution in sub-nodes Sub-nodes behave differently to other nodes when processing multiple items using an expression. Most nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn. In sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name. Node usage patterns # You can use the Zep Vector Store node in the following patterns. Use as a regular node to insert, update, and retrieve documents # You can use the Zep Vector Store as a regular node to insert or get documents. This pattern places the Zep Vector Store in the regular connection flow without using an agent. You can see an example of this in scenario 1 of this template (the example uses Supabase, but the pattern is the same). Connect directly to an AI agent as a tool # You can connect the Zep Vector Store node directly to the tool connector of an AI agent to use vector store as a resource when answering queries. Here, the connection would be: AI agent (tools connector) -> Zep Vector Store node. Use a retriever to fetch documents # You can use the Vector Store Retriever node with the Zep Vector Store node to fetch documents from the Zep Vector Store node. This is often used with the Question and Answer Chain node to fetch documents from the vector store that match the given chat input. An example of the connection flow (the example uses Pinecone, but the pattern in the same) would be: Question and Answer Chain (Retriever connector) -> Vector Store Retriever (Vector Store connector) -> Zep Vector Store. Use the Vector Store Question Answer Tool to answer questions # Another pattern uses the Vector Store Question Answer Tool to summarize results and answer questions from the Zep Vector Store node. Rather than connecting the Zep Vector Store directly as a tool, this pattern uses a tool specifically designed to summarizes data in the vector store. The connections flow (this example uses Supabase, but the pattern is the same) in this case would look like this: AI agent (tools connector) -> Vector Store Question Answer Tool (Vector Store connector) -> Zep Vector store. Node parameters # Operation Mode # This Vector Store node has four modes: Get Many , Insert Documents , Retrieve Documents (As Vector Store for Chain/Tool) , and Retrieve Documents (As Tool for AI Agent) . The mode you select determines the operations you can perform with the node and what inputs and outputs are available. Get Many # In this mode, you can retrieve multiple documents from your vector database by providing a prompt. The prompt will be embedded and used for similarity search. The node will return the documents that are most similar to the prompt with their similarity score. This is useful if you want to retrieve a list of similar documents and pass them to an agent as additional context. Insert Documents # Use Insert Documents mode to insert new documents into your vector database. Retrieve Documents (As Vector Store for Chain/Tool) # Use Retrieve Documents (As Vector Store for Chain/Tool) mode with a vector-store retriever to retrieve documents from a vector database and provide them to the retriever connected to a chain. In this mode you must connect the node to a retriever node or root node. Retrieve Documents (As Tool for AI Agent) # Use Retrieve Documents (As Tool for AI Agent) mode to use the vector store as a tool resource when answering queries. When formulating responses, the agent uses the vector store when the vector store name and description match the question details. Insert Documents parameters # Collection Name : Enter the collection name to store the data in. Get Many parameters # Collection Name : Enter the collection name to retrieve the data from. Prompt : Enter the search query. Limit : Enter how many results to retrieve from the vector store. For example, set this to 10 to get the ten best results. Retrieve Documents (As Vector Store for Chain/Tool) parameters # Collection Name : Enter the collection name to retrieve the data from. Retrieve Documents (As Tool for AI Agent) parameters # Name : The name of the vector store. Description : Explain to the LLM what this tool does. A good, specific description allows LLMs to produce expected results more often. Collection Name : Enter the collection name to retrieve the data from. Limit : Enter how many results to retrieve from the vector store. For example, set this to 10 to get the ten best results. Node options # Embedding Dimensions # Must be the same when embedding the data and when querying it. This sets the size of the array of floats used to represent the semantic meaning of a text document. Read more about Zep embeddings in Zep's embeddings documentation . Is Auto Embedded # Available in the Insert Documents Operation Mode, enabled by default. Disable this to configure your embeddings in Zep instead of in n8n. Metadata Filter # Available in Get Many mode. When searching for data, use this to match with metadata associated with the document. This is an AND query. If you specify more than one metadata filter field, all of them must match. When inserting data, the metadata is set using the document loader. Refer to Default Data Loader for more information on loading documents. Templates and examples # Building Your First WhatsApp Chatbot by Jimleuk View template details Ask questions about a PDF using AI by David Roberts View template details Chat with PDF docs using AI (quoting sources) by David Roberts View template details Browse Zep Vector Store integration templates , or search all templates Related resources # Refer to LangChain's Zep documentation for more information about the service. View n8n's Advanced AI documentation. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/openai-functions-agent/

OpenAI Functions Agent node # Use the OpenAI Functions Agent node to use an OpenAI functions model . These are models that detect when a function should be called and respond with the inputs that should be passed to the function. Refer to AI Agent for more information on the AI Agent node itself. You can use this agent with the Chat Trigger node. Attach a memory sub-node so that users can have an ongoing conversation with multiple queries. Memory doesn't persist between sessions. OpenAI Chat Model required You must use the OpenAI Chat Model with this agent. Node parameters # Configure the OpenAI Functions Agent using the following parameters. Prompt # Select how you want the node to construct the prompt (also known as the user's query or input from the chat). Choose from: Take from previous node automatically : If you select this option, the node expects an input from a previous node called chatInput . Define below : If you select this option, provide either static text or an expression for dynamic content to serve as the prompt in the Prompt (User Message) field. Require Specific Output Format # This parameter controls whether you want the node to require a specific output format. When turned on, n8n prompts you to connect one of these output parsers to the node: Auto-fixing Output Parser Item List Output Parser Structured Output Parser Node options # Refine the OpenAI Functions Agent node's behavior using these options: System Message # If you'd like to send a message to the agent before the conversation starts, enter the message you'd like to send. Use this option to guide the agent's decision-making. Max Iterations # Enter the number of times the model should run to try and generate a good answer from the user's prompt. Defaults to 10 . Return Intermediate Steps # Select whether to include intermediate steps the agent took in the final output (turned on) or not (turned off). This could be useful for further refining the agent's behavior based on the steps it took. Templates and examples # Refer to the main AI Agent node's Templates and examples section. Common issues # For common questions or issues and suggested solutions, refer to Common issues . AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/plan-execute-agent/

Plan and Execute Agent node # The Plan and Execute Agent is like the ReAct agent but with a focus on planning. It first creates a high-level plan to solve the given task and then executes the plan step by step. This agent is most useful for tasks that require a structured approach and careful planning. Refer to AI Agent for more information on the AI Agent node itself. Node parameters # Configure the Plan and Execute Agent using the following parameters. Prompt # Select how you want the node to construct the prompt (also known as the user's query or input from the chat). Choose from: Take from previous node automatically : If you select this option, the node expects an input from a previous node called chatInput . Define below : If you select this option, provide either static text or an expression for dynamic content to serve as the prompt in the Prompt (User Message) field. Require Specific Output Format # This parameter controls whether you want the node to require a specific output format. When turned on, n8n prompts you to connect one of these output parsers to the node: Auto-fixing Output Parser Item List Output Parser Structured Output Parser Node options # Refine the Plan and Execute Agent node's behavior using these options: Human Message Template # Enter a message that n8n will send to the agent during each step execution. Available LangChain expressions: {previous_steps} : Contains information about the previous steps the agent's already completed. {current_step} : Contains information about the current step. {agent_scratchpad} : Information to remember for the next iteration. Templates and examples # Refer to the main AI Agent node's Templates and examples section. Common issues # For common questions or issues and suggested solutions, refer to Common issues . AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/react-agent/

ReAct AI Agent node # The ReAct Agent node implements ReAct logic. ReAct (reasoning and acting) brings together the reasoning powers of chain-of-thought prompting and action plan generation. The ReAct Agent reasons about a given task, determines the necessary actions, and then executes them. It follows the cycle of reasoning and acting until it completes the task. The ReAct agent can break down complex tasks into smaller sub-tasks, prioritise them, and execute them one after the other. Refer to AI Agent for more information on the AI Agent node itself. No memory The ReAct agent doesn't support memory sub-nodes. This means it can't recall previous prompts or simulate an ongoing conversation. Node parameters # Configure the ReAct Agent using the following parameters. Prompt # Select how you want the node to construct the prompt (also known as the user's query or input from the chat). Choose from: Take from previous node automatically : If you select this option, the node expects an input from a previous node called chatInput . Define below : If you select this option, provide either static text or an expression for dynamic content to serve as the prompt in the Prompt (User Message) field. Require Specific Output Format # This parameter controls whether you want the node to require a specific output format. When turned on, n8n prompts you to connect one of these output parsers to the node: Auto-fixing Output Parser Item List Output Parser Structured Output Parser Node options # Use the options to create a message to send to the agent at the start of the conversation. The message type depends on the model you're using: Chat models : These models have the concept of three components interacting (AI, system, and human). They can receive system messages and human messages (prompts). Instruct models : These models don't have the concept of separate AI, system, and human components. They receive one body of text, the instruct message. Human Message Template # Use this option to extend the user prompt. This is a way for the agent to pass information from one iteration to the next. Available LangChain expressions: {input} : Contains the user prompt. {agent_scratchpad} : Information to remember for the next iteration. Prefix Message # Enter text to prefix the tools list at the start of the conversation. You don't need to add the list of tools. LangChain automatically adds the tools list. Suffix Message for Chat Model # Add text to append after the tools list at the start of the conversation when the agent uses a chat model. You don't need to add the list of tools. LangChain automatically adds the tools list. Suffix Message for Regular Model # Add text to append after the tools list at the start of the conversation when the agent uses a regular/instruct model. You don't need to add the list of tools. LangChain automatically adds the tools list. Return Intermediate Steps # Select whether to include intermediate steps the agent took in the final output (turned on) or not (turned off). This could be useful for further refining the agent's behavior based on the steps it took. Related resources # Refer to LangChain's ReAct Agents documentation for more information. Templates and examples # Refer to the main AI Agent node's Templates and examples section. Common issues # For common questions or issues and suggested solutions, refer to Common issues . AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/

Root nodes # Root nodes are the foundational nodes within a group of cluster nodes. Cluster nodes are node groups that work together to provide functionality in an n8n workflow. Instead of using a single node, you use a root node and one or more sub-nodes that extend the functionality of the node. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/sql-agent/

SQL AI Agent node # The SQL Agent uses a SQL database as a data source. It can understand natural language questions, convert them into SQL queries, execute the queries, and present the results in a user-friendly format. This agent is valuable for building natural language interfaces to databases. Refer to AI Agent for more information on the AI Agent node itself. Node parameters # Configure the SQL Agent using the following parameters. Data Source # Choose the database to use as a data source for the node. Options include: MySQL : Select this option to use a MySQL database. Also select the Credential for MySQL . SQLite : Select this option to use a SQLite database. You must add a Read/Write File From Disk node before the Agent to read your SQLite file. Also enter the Input Binary Field name of your SQLite file coming from the Read/Write File From Disk node. Postgres : Select this option to use a Postgres database. Also select the Credential for Postgres . Postgres and MySQL Agents If you are using Postgres or MySQL , this agent doesn't support the credential tunnel options. Prompt # Select how you want the node to construct the prompt (also known as the user's query or input from the chat). Choose from: Take from previous node automatically : If you select this option, the node expects an input from a previous node called chatInput . Define below : If you select this option, provide either static text or an expression for dynamic content to serve as the prompt in the Prompt (User Message) field. Node options # Refine the SQL Agent node's behavior using these options: Ignored Tables # If you'd like the node to ignore any tables from the database, enter a comma-separated list of tables you'd like it to ignore. If left empty, the agent doesn't ignore any tables. Include Sample Rows # Enter the number of sample rows to include in the prompt to the agent. Default is 3 . Sample rows help the agent understand the schema of the database, but they also increase the number of tokens used. Included Tables # If you'd only like to include specific tables from the database, enter a comma-separated list of tables to include. If left empty, the agent includes all tables. Prefix Prompt # Enter a message you'd like to send to the agent before the Prompt text. This initial message can provide more context and guidance to the agent about what it can and can't do, and how to format the response. n8n fills this field with an example. Suffix Prompt # Enter a message you'd like to send to the agent after the Prompt text. Available LangChain expressions: {chatHistory} : A history of messages in this conversation, useful for maintaining context. {input} : Contains the user prompt. {agent_scratchpad} : Information to remember for the next iteration. n8n fills this field with an example. Limit # Enter the maximum number of results to return. Default is 10 . Templates and examples # Refer to the main AI Agent node's Templates and examples section. Common issues # For common questions or issues and suggested solutions, refer to Common issues . AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/

Sub nodes # Sub nodes attach to root nodes within a group of cluster nodes. They configure the overall functionality of the cluster. Cluster nodes are node groups that work together to provide functionality in an n8n workflow. Instead of using a single node, you use a root node and one or more sub-nodes that extend the functionality of the node. AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================

================================================================================
Source URL: https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/tools-agent/

Tools AI Agent node # The Tools Agent uses external tools and APIs to perform actions and retrieve information. It can understand the capabilities of different tools and determine which tool to use depending on the task. This agent helps integrate LLMs with various external services and databases. This agent has an enhanced ability to work with tools and can ensure a standard output format. The Tools Agent implements Langchain's tool calling interface. This interface describes available tools and their schemas. The agent also has improved output parsing capabilities, as it passes the parser to the model as a formatting tool. Refer to AI Agent for more information on the AI Agent node itself. You can use this agent with the Chat Trigger node. Attach a memory sub-node so that users can have an ongoing conversation with multiple queries. Memory doesn't persist between sessions. This agent supports the following chat models: OpenAI Chat Model Groq Chat Model Mistral Cloud Chat Model Anthropic Chat Model Azure OpenAI Chat Model The Tools Agent can use the following tools... Call n8n Workflow Code HTTP Request Action Network ActiveCampaign Affinity Agile CRM Airtable APITemplate.io Asana AWS Lambda AWS S3 AWS SES AWS Textract AWS Transcribe Baserow Bubble Calculator ClickUp CoinGecko Compression Crypto DeepL DHL Discord Dropbox Elasticsearch ERPNext Facebook Graph API FileMaker Ghost Git GitHub GitLab Gmail Google Analytics Google BigQuery Google Calendar Google Chat Google Cloud Firestore Google Cloud Realtime Database Google Contacts Google Docs Google Drive Google Sheets Google Slides Google Tasks Google Translate Google Workspace Admin Gotify Grafana GraphQL Hacker News Home Assistant HubSpot Jenkins Jira Software JWT Kafka LDAP Line LinkedIn Mailcheck Mailgun Mattermost Mautic Medium Microsoft Excel 365 Microsoft OneDrive Microsoft Outlook Microsoft SQL Microsoft Teams Microsoft To Do Monday.com MongoDB MQTT MySQL NASA Nextcloud NocoDB Notion Odoo OpenWeatherMap Pipedrive Postgres Pushover QuickBooks Online QuickChart RabbitMQ Reddit Redis RocketChat S3 Salesforce Send Email SendGrid SerpApi (Google Search) Shopify Slack Spotify Stripe Supabase Telegram Todoist TOTP Trello Twilio urlscan.io Vector Store Webflow Wikipedia Wolfram|Alpha WooCommerce Wordpress X (Formerly Twitter) YouTube Zendesk Zoho CRM Zoom Node parameters # Configure the Tools Agent using the following parameters. Prompt # Select how you want the node to construct the prompt (also known as the user's query or input from the chat). Choose from: Take from previous node automatically : If you select this option, the node expects an input from a previous node called chatInput . Define below : If you select this option, provide either static text or an expression for dynamic content to serve as the prompt in the Prompt (User Message) field. Require Specific Output Format # This parameter controls whether you want the node to require a specific output format. When turned on, n8n prompts you to connect one of these output parsers to the node: Auto-fixing Output Parser Item List Output Parser Structured Output Parser Node options # Refine the Tools Agent node's behavior using these options: System Message # If you'd like to send a message to the agent before the conversation starts, enter the message you'd like to send. Use this option to guide the agent's decision-making. Max Iterations # Enter the number of times the model should run to try and generate a good answer from the user's prompt. Defaults to 10 . Return Intermediate Steps # Select whether to include intermediate steps the agent took in the final output (turned on) or not (turned off). This could be useful for further refining the agent's behavior based on the steps it took. Automatically Passthrough Binary Images # Use this option to control whether binary images should be automatically passed through to the agent as image type messages (turned on) or not (turned off). Templates and examples # Refer to the main AI Agent node's Templates and examples section. Dynamic parameters for tools with $fromAI() # To learn how to dynamically populate parameters for app node tools, refer to Let AI specify tool parameters with $fromAI() . Common issues # For common questions or issues and suggested solutions, refer to Common issues . AI glossary # completion : Completions are the responses generated by a model like GPT. hallucinations : Hallucination in AI is when an LLM (large language model) mistakenly perceives patterns or objects that don't exist. vector database : A vector database stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. vector store : A vector store, or vector database, stores mathematical representations of information. Use with embeddings and retrievers to create a database that your AI can access when answering questions. Was this page helpful? Thanks for your feedback! Thanks for your feedback! Help us improve this page by submitting an issue or a fix in our GitHub repo . Back to top
================================================================================
